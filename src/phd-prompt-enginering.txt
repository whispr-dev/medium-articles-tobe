






...







Chapter 2 â€” Background and Related Work
2.0 Purpose and scope

This thesis asks whether prompt engineering can be formalized as an information-theoretic control process: prompts act as control inputs that shape a modelâ€™s trajectory distribution during decoding, and â€œgoodâ€ prompt structures are those that increase task-relevant information while reducing spurious uncertainty (hallucination) across model scales. The goal of this chapter is to position that claim within prior work across four themes: (i) prompt patterns and compositional prompting, (ii) in-context learning (ICL) theory, (iii) program and grammar induction parallels, and (iv) robustness and causal evaluation methodology. Together, these strands motivate treating prompts not as ad hoc strings but as structured, composable objects whose effects can be measured, optimized, and tested for invariance.

2.1 Prompting as empirical control: strategies, compositional prompting, and prompt â€œmovesâ€

Prompting research initially emphasized the surprising ability of large language models (LLMs) to perform tasks from natural language instructions and a small number of in-context examples. Over time, the field converged on a set of repeatable â€œmovesâ€ that systematically improve outcomes. These moves can be interpreted as control primitives that steer generation by reshaping conditional distributions over trajectories.

2.1.1 Instruction prompting and few-shot conditioning

Instruction prompting and few-shot prompting operate by providing a task description and, optionally, demonstrations in the prompt context. The demonstrations function as implicit training data: they provide the model with a pattern to imitate, and they supply an inductive bias about the correct mapping from inputs to outputs. Although simple, few-shot prompting is a canonical example of control via conditioning: it modifies the modelâ€™s posterior behavior without parameter updates.

2.1.2 Decomposition and trajectory scaffolding

A major advance in prompting came from prompts that explicitly request intermediate reasoning steps (chain-of-thought prompting) or multi-stage decomposition. In control terms, these techniques introduce structured intermediate states that constrain the trajectory space: rather than directly sampling an answer, the model samples a reasoning path and then an answer conditioned on that path. Even when the intermediate reasoning is not formally verified, the pattern frequently improves accuracy on multi-step tasks, suggesting that prompting can change not only outputs but the internal computation the model performs before committing to a final token sequence.

2.1.3 Stochastic decoding and posterior aggregation

A complementary line of work recognizes that single-shot decoding is often a poor estimator of correctness. Self-consistency and related methods sample multiple reasoning trajectories and aggregate final answers by consensus. This is naturally interpreted as a distributional control technique: rather than seeking a single â€œbestâ€ trajectory, the method approximates marginalization over trajectories and uses agreement as a proxy for posterior mass. From an information-theoretic viewpoint, this can reduce uncertainty attributable to sampling noise or brittle reasoning paths, but it can also fail when the modelâ€™s errors are systematic and consensus is wrong. These methods motivate measuring not only mean performance but also variance, disagreement rates, and entropy over outputs.

2.1.4 Closed-loop prompting with tool use and observation

ReAct and tool-augmented prompting frameworks interleave reasoning with actions (retrieval, calculation, code execution) and incorporate observations back into the prompt. This resembles feedback control: the modelâ€™s intermediate state is corrected using external signals, reducing reliance on purely internal generation. These frameworks are especially relevant to hallucination because they enable groundingâ€”answers can be conditioned on retrieved evidence rather than latent associations. The existence and success of closed-loop prompting supports the thesisâ€™s framing: prompting can be treated as a policy operating over a trajectory distribution, and tool observations change that distribution in measurable ways.

2.1.5 Structured output prompting and constrained decoding

A practical and theoretically important subset of prompt engineering concerns structured outputs: JSON schemas, DSLs, SQL, and other formally constrained formats. Here, prompt structure is often paired with constrained decoding (e.g., regex, CFG, JSON-schema enforcement) to restrict the support of the modelâ€™s output distribution. This is an explicit entropy-control mechanism: by construction, invalid outputs are excluded. However, constrained decoding can introduce tradeoffs: restricting support can reduce syntactic errors while potentially increasing semantic errors if the constraint prevents the model from expressing uncertainty or nuance. These tensions motivate a separation between format validity and task correctness in evaluation.

2.2 Prompt patterns and taxonomies: vocabulary without a unifying objective

Prompt pattern catalogs and surveys provide a shared vocabulary for describing prompts as compositions of functional blocks: role assignment, goal statements, constraints, exemplars, output schemas, and self-verification instructions. This literature is valuable because it identifies recurring prompt structures and provides candidate variables for systematic study. However, taxonomies are largely descriptive: they explain what kinds of prompts exist rather than why particular structures work, and they rarely specify causal mechanisms or transferable invariants.

For this thesis, the key contribution of the taxonomy literature is to motivate a representation of prompts as compositional objects. If prompts can be decomposed into a small set of functional blocks, then the central questions become: which blocks (or block compositions) are responsible for performance improvements; whether their effects are stable across tasks and model families; and how they change uncertainty and hallucination risk.

2.3 In-context learning theory: prompts as inference and implicit learning

Prompting is closely tied to in-context learning: the ability of a model to adapt behavior from examples in its context window without parameter updates. A growing theory literature proposes that ICL can be understood as a form of implicit inference over latent task variables.

2.3.1 ICL as implicit Bayesian inference

One influential account frames ICL as approximate Bayesian inference: the model uses the prompt (instructions plus examples) as evidence about a latent concept, rule, or mapping, and it conditions predictions on an inferred posterior over that latent variable. This perspective is crucial for an information-theoretic formalization because it introduces a latent objectâ€”call it 
ğ‘
Zâ€”representing task-relevant structure (label mapping, format constraints, domain assumptions). If prompting causes the model to infer 
ğ‘
Z, then â€œgood promptsâ€ are those that convey maximal information about 
ğ‘
Z and suppress irrelevant uncertainty.

2.3.2 Bayesian limits, scaling effects, and partial validity

More recent work tests how far the Bayesian framing can be pushed and where it breaks. Empirically, transformer ICL sometimes behaves like Bayesian model averaging over hypotheses consistent with the prompt, but the match is imperfect and can depend on model scale, training distribution, and prompt form. This nuance matters: it suggests that invariants may exist only within certain regimes, and that â€œoptimal prompt structureâ€ may be scale-dependent. A thesis centered on invariants must therefore treat scaling not as an afterthought but as a primary axis of evaluation.

2.3.3 Implication for prompt structure

ICL theory typically treats the prompt as a sequence of tokens, but it provides a target for structural analysis. If the promptâ€™s purpose is to identify 
ğ‘
Z, then prompt blocks can be interpreted as different channels for communicating information about 
ğ‘
Z: demonstrations convey inputâ€“output mapping; constraints convey admissible outputs; schemas convey a formal language; verifiers induce a preference for self-consistency. This motivates defining â€œtask-relevant mutual informationâ€ using proxies tied to 
ğ‘
Z-recovery and predictive reliability, rather than relying solely on raw accuracy.

2.4 Program and grammar parallels: prompts as programs, not strings

A parallel research ecosystem treats prompting as a form of programming. Prompt-programming languages and constrained generation frameworks formalize prompts as executable artifacts: they combine natural language with control flow, constraints, and structured templates.

2.4.1 Prompt-as-program languages

Systems such as LMQL and Guidance embed prompts in a programming environment, enabling loops, conditionals, variable binding, and constraints (regex/CFG/JSON). This reframes prompt engineering as designing a small program that orchestrates model calls and constrains outputs. For this thesis, the importance is not the particular tool but the conceptual shift: once prompting is programmable, prompt structure becomes explicit and can be optimized.

2.4.2 Constrained decoding as support restriction

Frameworks like Outlines enforce schemas or grammars at generation time. From the thesisâ€™s perspective, these methods implement a direct manipulation of the output distributionâ€™s support: invalid sequences are assigned probability zero under the constrained decoder. This provides a clean operational path to â€œentropy minimization,â€ but it also clarifies what entropy is being reduced: syntactic entropy over invalid formats, not necessarily semantic uncertainty about task facts. That distinction is central to defining hallucination entropy.

2.4.3 Optimization and compilation: toward â€œoptimal prompt structuresâ€

DSPy and related optimization frameworks treat prompt components as parameters to be tuned, searched, or compiled. This is a practical bridge from heuristic prompt engineering to objective-driven design. It also exposes a missing theoretical piece: optimization systems can find prompts that perform well, but they do not by themselves explain why a structure transfers across tasks or scales, nor do they identify invariants. A formal framework needs to connect the discovered structures to stable, testable properties.

2.5 Robustness and evaluation methodology: prompt sensitivity, perturbations, and causal claims

A persistent problem in prompt engineering research is sensitivity: small prompt changes can cause large outcome changes. This undermines naive claims of improvement and makes it easy to overfit to a benchmark through prompt tuning.

2.5.1 Prompt sensitivity and robustness benchmarks

Recent evaluation tools (e.g., PromptBench) treat prompt perturbations as first-class adversarial conditions: paraphrase, instruction injection, distractors, and block reordering. This literature establishes that robust prompting must be evaluated under systematic perturbations, and it provides ready-made stress tests for invariance claims.

2.5.2 Multi-metric evaluation and tradeoffs

Frameworks like HELM emphasize that â€œbetterâ€ depends on the metric: accuracy, robustness, calibration, toxicity, and efficiency can trade off. For this thesis, multi-metric evaluation is not just best practice; it is required by the information-theoretic framing. Maximizing task-relevant information while minimizing hallucination entropy implies joint objectives and explicit tradeoff reporting.

2.5.3 Causal evaluation via prompt interventions

Descriptive ablations are insufficient to claim invariants. To assert that a structural block causes an improvement, evaluations should use counterfactual prompt edits: hold wording fixed while changing structure; hold structure fixed while paraphrasing; and evaluate across seeds and models. Distributional reportingâ€”means plus variance, worst-case performance under perturbations, and entropy-like measuresâ€”reduces the chance of cherry-picked prompt success and aligns with the thesisâ€™s trajectory-distribution viewpoint.

2.6 Synthesis: what the literature establishes and what remains unresolved

Prior work establishes that prompting can reliably improve performance through a set of reusable strategies; that ICL can be partially explained as inference over latent task variables; that prompt programming and constrained decoding make structure explicit and enforceable; and that robust evaluation requires perturbation testing and distributional reporting. However, the field lacks a unifying formal framework that links prompt structure to measurable information flow and uncertainty control across model scales.

This thesis addresses that gap by (i) representing prompts as compositions of functional operators, (ii) modeling prompting as control over trajectory distributions, (iii) defining objective functions grounded in task-relevant information and hallucination-related uncertainty, and (iv) empirically testing invariants across tasks, perturbations, and model scales using standardized evaluation harnesses.







...






Literature review chapter skeleton (headings + argument flow)
2.0 Chapter objective and framing

Restate the research question as a formalization problem: prompting as a control input that shapes an LLMâ€™s trajectory distribution during decoding, with objectives expressed via information (task-relevant mutual information) and uncertainty (hallucination entropy).

Preview what the chapter will establish:

empirical prompt â€œcontrol primitivesâ€

theoretical accounts of ICL as inference/learning

program/grammar analogies that make â€œprompt = policy/programâ€ precise

evaluation/robustness literature showing why naive prompt comparisons are invalid

2.1 Prompting as trajectory control: empirically successful control primitives

Argument flow: show that todayâ€™s strongest prompting methods are already doing control, just without formal objective functions.

2.1.1 Decomposition and intermediate-state scaffolding

Chain-of-thought prompting as explicit latent state exposure / trajectory scaffolding

Chaining prompts into modular multi-step pipelines (â€œAI Chainsâ€) as user-visible control graphs

2.1.2 Stochastic decoding and posterior aggregation

Self-consistency as â€œsample trajectories then marginalize answersâ€ (implicit entropy management)

2.1.3 Closed-loop control with environment feedback

ReAct as an explicit interleaving of reasoning and action that reduces hallucination via external observations (feedback control)

2.1.4 Prompt optimization / compilation (toward â€œoptimal structureâ€)

Automatic Prompt Engineer (APE): treat instruction as a â€œprogramâ€ and search for one maximizing a score

DSPy: prompt compilation + optimization algorithms as a practical bridge from heuristics to objective-driven design

2.1.5 Constrained output support (entropy reduction by construction)

Grammar prompting (BNF/CFG constraints) and structured outputs as explicit support restriction

Transition: These methods look like control policies, but the field lacks a unifying objective: what are we optimizing, and what is invariant across models/scales?

2.2 In-context learning theory: prompts as implicit training data / inference

Argument flow: connect â€œprompt as controlâ€ to â€œprompt as evidence,â€ enabling information-theoretic objectives.

2.2.1 GPT-3 and the emergence of ICL

Few-shot learning via conditioning on examples; sets the stage for â€œprompt = dataâ€ framing

2.2.2 ICL as implicit Bayesian inference

Xie et al.: ICL emerges when models infer latent concepts; formal proof in a mixture/HMM setting

â€œBayesian Prismâ€: how far Bayesian framing explains transformer ICL; where it matches/fails

(Optional add) recent finite-sample/statistical ICL theory threads can be cited as the field moves toward risk decompositions

Transition: If prompts induce a posterior over latent task concepts, we can formalize â€œgood promptsâ€ as those that maximize task-relevant information and minimize spurious uncertainty.

2.3 Prompt patterns and taxonomies: describing structure before explaining it

Argument flow: taxonomies provide vocabulary for prompt structure, but not causal guarantees.

Prompt patterns catalog (pattern-language approach) â€” useful ontology of â€œmovesâ€ and compositional patterns

Prompt Report: systematic survey + shared terminology (what exists; fragmentation problem)

Bridge to your thesis: taxonomies suggest candidate structural variables 
ğ‘†
(
ğ‘ƒ
)
S(P) (roles, constraints, exemplars, verifiers, output schemas) â€” youâ€™ll convert these into variables in an information-theoretic control objective.

2.4 Program induction / formal languages parallels: prompts as programs, not strings

Argument flow: show that once prompting becomes â€œexecutable structure,â€ we can talk about compilation, invariants, and constraints formally.

2.4.1 Prompt-as-program languages

LMQL (Programming Large Language Models): declarative constraints + control flow + prompt text

Guidance: interleave control (loops/conditionals/tools) with generation; regex/CFG constraints

Outlines: typed/schema constrained decoding (structured output guarantees)

2.4.2 Constrained decoding as entropy control

Constrained decoding resources (surveyed lists) â€” situate your work in â€œreliable output distribution shapingâ€

Transition: Program-like representations let you define â€œprompt structureâ€ as an object that can be optimized and compared across model scales.

2.5 Robustness, evaluation, and causal methodology: why naive prompt comparisons fail

Argument flow: motivate why you need robust, distributional evaluation + causal interventions to claim anything like â€œmutual information improvementâ€ or â€œhallucination entropy reduction.â€

2.5.1 Robustness benchmarks and prompt attacks

PromptBench: unified evaluation library with adversarial prompt attacks + protocols

2.5.2 Holistic, multi-metric evaluation frameworks

HELM: scenario+metric taxonomy and standardized evaluation; highlights tradeoffs beyond accuracy

2.5.3 Reproducibility + sensitivity

Emphasize reporting distributions over prompt variants and decoding seeds (connect to your â€œentropyâ€ framing). (This section sets up your experimental design chapter.)

2.6 Synthesis: whatâ€™s known, whatâ€™s missing, and where your formalism lands

Known: prompt strategies are effective control heuristics; ICL has inference-theoretic explanations; programmatic prompting exists; robustness evaluation is improving.

Missing (your contribution): a formal objective + invariants tying prompt structure to information flow and uncertainty/hallucination across model scales.

Research Matrix A â€” â€œpaper â†’ thesis sub-claimâ€ mapping

Below is a compact matrix you can lift straight into your thesis planning docs.

Thesis sub-claim (what youâ€™ll argue)	Key sources	What they contribute (usable â€œevidenceâ€)	How youâ€™ll use it
Prompting can elicit structured multi-step trajectories that improve reasoning	Chain-of-Thought	Demonstrates performance gains from explicit intermediate reasoning steps	Motivate â€œtrajectory shapingâ€ and define trajectory variables
Sampling multiple trajectories and aggregating reduces error (uncertainty management)	Self-Consistency	Replace greedy decoding with sampling+marginalization; boosts reasoning	Prototype for â€œentropyâ€ / â€œposterior aggregationâ€ mechanisms
Feedback (actions/observations) reduces hallucination vs pure reasoning	ReAct	Interleaves reasoning and acting; uses observations to correct errors	Basis for closed-loop control framing + hallucination reduction
Multi-step prompt pipelines increase controllability and debuggability	AI Chains	Defines primitives; user study shows transparency/control improvements	Supports â€œprompt = modular control policy,â€ not a flat string
In-context learning can be interpreted as latent concept inference (Bayesian-ish)	Implicit Bayesian ICL	Formal setting where ICL â‰ˆ Bayesian inference over latent concepts	Enables information-theoretic objective: prompts as evidence
Bayesian framing matches transformer behaviour in some regimes; clarifies inductive bias	Bayesian Prism	Empirical comparison between Bayesian predictors and transformers	Use to delimit where theory holds + how scaling affects it
Prompt engineering space is fragmented; needs unified vocabulary/taxonomy	Prompt Report	Taxonomy and terminology; map of technique landscape	Provides your â€œstructural variablesâ€ candidate list
Prompts can be â€œoptimizedâ€ via search like program synthesis	APE	Treat instruction as program; generate candidates; select by score	Precedent for objective-driven prompt structure optimization
Prompting should be treated as programming with constraints/control flow	LMQL + Guidance	Languages that combine prompts with constraints and control	Your formalism can target these representations
Constrained decoding / structured outputs restrict output support (lower entropy, higher validity)	Outlines + constrained decoding list	Practical methods to ensure schema/grammar compliance	Operationalize entropy reduction by support restriction
Prompt robustness must be evaluated under perturbations/attacks	PromptBench	Attack suite + evaluation harness	Your invariants must survive these perturbations
â€œGoodâ€ prompting claims require multi-metric transparency	HELM	Scenario/metric taxonomy; standardized prompts/completions	Your evaluation must be multi-metric and scenario-aware
Prompt optimization frameworks can compile prompts from modular programs	DSPy	Optimizers for prompts/weights; modular program abstraction	A baseline system for â€œderive optimal prompt structuresâ€
Research Matrix B â€” â€œtool â†’ experimentâ€ mapping

This matrix is the backbone of your Methods/Experiments chapters.

Experiment module (what youâ€™ll run)	Primary tool(s)	What it produces	Why it matters for your question
E1: Prompt structure ablations (role/goal/constraints/examples/schema/verifier blocks)	LMQL / Guidance	Controlled structural edits; constraint-aware generation	Lets you treat prompt structure as compositional operators
E2: Support restriction vs post-hoc validation	Outlines (and optionally Guardrails later)	Validity rates, format error rates, retries	Measures entropy reduction via constrained support (not just retries)
E3: Robustness under prompt attacks/perturbations	PromptBench	Performance distributions under attack types	Tests whether â€œinvariantsâ€ are actually invariant
E4: Multi-metric evaluation across scenarios (accuracy, calibration, robustness, efficiency, etc.)	HELM	ScenarioÃ—metric grid; standardized prompts/completions	Prevents â€œimproves accuracy but worsens toxicity/robustnessâ€ blindspots
E5: Automatic prompt structure search / compilation	DSPy + APE	Learned prompt templates/programs optimizing your objective	Directly targets â€œderive optimal prompt structuresâ€
E6: Trajectory sampling & aggregation (entropy and stability)	Self-consistency-style pipelines	Variance across trajectories; consensus stability	Direct operational handle on â€œtrajectory entropyâ€
E7: Closed-loop control against hallucinations	ReAct-style pipelines	Hallucination reduction with observation feedback	Tests open-loop vs closed-loop control hypotheses
E8: Scaling-law tests (same structure across model sizes/families)	HELM harness + your runner	Cross-model transfer curves	Answers the â€œacross model scalesâ€ part explicitly
Where your â€œmissing definitions/theoremâ€ belong in the formal framework

Put these in Chapter 4 (Formal Framework), before you present experiments. The lit review motivates them; the framework operationalizes them.

4.1 Definitions (place immediately after preliminaries)

Trajectory distribution

Define a decoding trajectory 
ğœ
=
(
ğ‘¦
1
,
â€¦
,
ğ‘¦
ğ‘‡
)
Ï„=(y
1
	â€‹

,â€¦,y
T
	â€‹

) and induced distribution 
ğ‘
ğœƒ
(
ğœ
âˆ£
ğ‘ƒ
,
ğ‘¥
,
ğ‘‘
)
p
Î¸
	â€‹

(Ï„âˆ£P,x,d) where:

ğ‘ƒ
P = prompt (control input)

ğ‘¥
x = task input

ğ‘‘
d = decoding policy (temperature, nucleus, etc.)

ğœƒ
Î¸ = model parameters

Prompt as control policy

Treat prompt structure as 
ğ‘ƒ
=
ğ¶
(
ğ‘ 
1
,
â€¦
,
ğ‘ 
ğ‘˜
)
P=C(s
1
	â€‹

,â€¦,s
k
	â€‹

) where 
ğ‘ 
ğ‘–
s
i
	â€‹

 are compositional blocks (role, constraints, exemplars, schema, verifierâ€¦).

This is where Prompt Report / pattern catalogs feed your ontology

Task-relevant latent variable

Define 
ğ‘
Z as the latent â€œtask conceptâ€ or â€œruleâ€ the model must infer (label mapping, format constraints, domain assumptions). This is exactly the Bayesian ICL bridge

Hallucination entropy (operational)

Define a measurable proxy, e.g. entropy in unsupported content under verification, or divergence from a reference constraint set.

4.2 Theorem/Proposition slots (you likely want 2â€“3)

These are â€œthesis spineâ€ statements â€” youâ€™ll prove them in idealized settings and validate empirically in realistic settings.

Proposition A (Control Decomposition):
If prompt is composed of blocks 
ğ‘ 
ğ‘–
s
i
	â€‹

 with separable semantic functions, then the effect on 
ğ‘
ğœƒ
(
ğ‘¦
âˆ£
ğ‘ƒ
,
ğ‘¥
)
p
Î¸
	â€‹

(yâˆ£P,x) can be approximated by a composition of operators on a latent state distribution.

This is where LMQL/Guidance â€œprompt-as-programâ€ is your motivating prior

Proposition B (Information Objective):
Under a latent concept model 
ğ‘
Z, an â€œoptimal promptâ€ maximizes a lower bound on 
ğ¼
(
ğ‘
;
ğ‘Œ
âˆ£
ğ‘‹
,
ğ‘ƒ
)
I(Z;Yâˆ£X,P) while minimizing an upper bound on spurious entropy 
ğ»
(
ğ‘Œ
unsupported
âˆ£
ğ‘‹
,
ğ‘ƒ
)
H(Y
unsupported
	â€‹

âˆ£X,P).

Literature anchor: ICL as latent concept inference

Proposition C (Constraint as Support Restriction):
Hard constraints (CFG/schema) reduce output entropy by restricting the support of 
ğ‘
ğœƒ
p
Î¸
	â€‹

, and should strictly improve validity metrics relative to post-hoc validation when constraints match the task language.

Literature anchor: constrained decoding tooling

4.3 Experimental hypotheses (right after propositions)

H1: adding schema+verifier blocks increases task-relevant MI proxies and decreases hallucination proxies

H2: constrained decoding yields lower â€œunsupported entropyâ€ than retry-based validators at equal cost

H3: invariants persist across scales only when they align with the ICL latent concept 
ğ‘
Z (Bayesian view)

What I need from your â€œother crucial papersâ€ bundle (already handled)

You said you grabbed some extra crucial papers â€” your uploaded review.zip includes several big ones (GPT-3, AI Chains, APE, etc.), and weâ€™ve integrated them into the skeleton + matrices via their canonical sources (arXiv / venue / GitHub).







...


this might not be no.1


0) Experimental goal and core hypotheses
Goal

Model prompting as a control input that shapes the modelâ€™s trajectory distribution 
ğ‘
ğœƒ
(
ğœ
âˆ£
ğ‘ƒ
,
ğ‘¥
,
ğ‘‘
)
p
Î¸
	â€‹

(Ï„âˆ£P,x,d), and test whether structured prompts can be optimized to:

increase task-relevant information (proxy for higher 
ğ¼
(
ğ‘
;
ğ‘Œ
âˆ£
ğ‘‹
,
ğ‘ƒ
)
I(Z;Yâˆ£X,P))

decrease hallucination entropy / unsupported content (proxy for lower spurious uncertainty)

generalize across model scales/families

Hypotheses

H1 (Structure â†’ Information): Adding structure blocks (schema + constraints + verifier) increases task success and reduces output variance across seeds/prompts.

H2 (Support restriction â†’ Entropy): Hard constraints (CFG/schema constrained decoding) reduce â€œunsupported contentâ€ entropy more than post-hoc validation/retry loops at comparable token budgets.

H3 (Closed-loop â†’ Hallucination): ReAct-style feedback control yields larger hallucination reductions than open-loop prompting for knowledge-grounded tasks.

H4 (Scale invariants): Some operators have stable positive effect across model scales; others are scale-dependent (interaction effects).

1) Datasets and task suite (balanced for structure, truthfulness, and scale)

Pick a task suite that spans: structured output, reasoning, grounding, robustness, and domain variation.

A. Structured output tasks (constraint-friendly)

Information extraction: entity / relation extraction into JSON

Text-to-SQL / DSL generation: strict grammar

Classification with schema output: label + confidence + rationale field (optional)

Public datasets (good defaults):

Spider (Text-to-SQL) (structured; grammar constraints matter)

CoNLL03 / ACE-style NER (schema JSON output)

BoolQ / AG News / TREC (classification + schema)

B. Reasoning tasks (trajectory-sensitive)

GSM8K (math reasoning)

SVAMP (math word problems)

StrategyQA / CommonsenseQA (commonsense + brittle prompting)

C. Knowledge-grounded / hallucination-sensitive tasks

TruthfulQA (hallucination pressure)

FEVER (claim verification; can add retrieval / citations)

Natural Questions (short answer) (with and without retrieval)

D. Robustness / adversarial prompt perturbations

Use PromptBench attack suite to perturb prompts and test invariance under:

instruction injection

paraphrase / reorder

distractor context

role confusion

E. HELM scenarios (for multi-metric coverage)

Use HELM to ensure youâ€™re not optimizing just â€œaccuracyâ€ while hurting robustness or calibration.

Design note: you want at least 3 task archetypes per group, and at least 2 domains per archetype, to avoid â€œone dataset tells the whole storyâ€ syndrome.

2) Models and scale grid

You need a scale grid with at least:

small (~7B)

medium (~13â€“34B)

large (~70B or frontier API models)

For each family, keep decoding policy constant per condition, then test sensitivity.

Scale design:

Factor 1: model size (S/M/L)

Factor 2: model family (at least 2 families if possible)

Factor 3: decoding regime (greedy vs sampled)

Your analysis will explicitly model size Ã— operator interactions.

3) Prompt operator set (your â€œcontrol primitivesâ€)

Define prompts as compositions of operators on a base task spec 
ğ‘ƒ
0
P
0
	â€‹

.

Base template 
ğ‘ƒ
0
P
0
	â€‹


Minimal prompt: â€œDo task T on input X.â€

Operators (compose them factorially)

Open-loop structure operators

ROLE: assign identity + stance (e.g., â€œYou are a careful verifierâ€¦â€)

GOAL: explicit objective statement

CONSTRAINTS: must/should rules (length, style, allowed sources)

SCHEMA: explicit output format (JSON schema / EBNF)

EXEMPLARS: few-shot examples (k = 0, 1, 4)

DECOMPOSE: step plan or checklist (optionally hidden)

VERIFIER: self-check / consistency check / cite evidence lines

ABSTAIN: explicit â€œIf unknown, say â€˜INSUFFICIENT INFOâ€™â€

Closed-loop operators
9. REACT_LOOP: actâ†’observeâ†’revise (with tools or retrieval)
10. RETRY_ON_FAIL: validate; if invalid, revise and resubmit

Support restriction operators
11. CFG_CONSTRAIN: grammar constrained decoding (Outlines/Guidance)
12. REGEX_CONSTRAIN: regex constrained output
13. TYPE_CONSTRAIN: typed JSON schema enforcement

Canonical operator bundles (recommended conditions)

Instead of full 
2
13
2
13
 factorial (too big), test these bundles:

C0: Baseline 
ğ‘ƒ
0
P
0
	â€‹


C1: ROLE + GOAL

C2: + CONSTRAINTS

C3: + SCHEMA (no hard constraint yet)

C4: + VERIFIER + ABSTAIN

C5: + EXEMPLARS (few-shot)

C6: SCHEMA + CFG_CONSTRAIN (hard support restriction)

C7: REACT_LOOP (with/without retrieval)

C8: DSPy-optimized prompt/program (learned structure)

This gives you a ladder from heuristics â†’ formal constraint â†’ optimization.

4) Metrics (task success + information proxies + entropy/hallucination proxies)

You need three metric families:

A. Task performance

Accuracy / F1 / Exact Match (dataset-specific)

SQL execution accuracy (Spider)

JSON validity rate (structured tasks)

B. â€œTask-relevant mutual informationâ€ proxies (operational MI)

True 
ğ¼
(
ğ‘
;
ğ‘Œ
)
I(Z;Y) is hard because 
ğ‘
Z (latent task concept) isnâ€™t directly observed, so use proxies that correlate with â€œinformation about task-relevant factorsâ€:

Proxy 1: Minimal Description Length (MDL) / Compression score

Treat prompt as encoding of task rule; evaluate how well outputs predict labels with minimal extra bits.

Operationally: train a lightweight probe on outputs to predict ground truth; MDL decreases â†’ more task-relevant info.

Proxy 2: Contrastive separability of outputs

Given pairs 
(
ğ‘¥
,
ğ‘¦
âˆ—
)
(x,y
âˆ—
) and 
(
ğ‘¥
,
ğ‘¦
~
)
(x,
y
~
	â€‹

), score whether model outputs under prompt align closer to 
ğ‘¦
âˆ—
y
âˆ—
 than distractors.

Use an InfoNCE-like objective computed over outputs in embedding space.

Proxy 3: Calibration / confidence alignment (where available)

ECE (Expected Calibration Error) if you can get confidence; otherwise use self-reported confidence with care.

Better prompts should improve calibration on tasks with abstain option.

C. Hallucination / â€œunsupported entropyâ€ proxies

Pick auditable hallucination metrics:

Hallu Proxy 1: Unsupported claim rate (UCR)

Define â€œclaimsâ€ as extracted triples or named entities; mark unsupported if absent from provided context / retrieval docs.

Measure: unsupported claims per 100 tokens.

Hallu Proxy 2: Abstention correctness

On unanswerable items: does model abstain vs fabricate?

Hallu Proxy 3: Self-contradiction rate

Run verifier pass: consistency check across answer segments.

Hallu Proxy 4: Output entropy / variance across seeds

Sample N trajectories per prompt condition; compute:

answer disagreement rate

variance in key fields

entropy over categorical outputs

This connects directly to â€œtrajectory distribution stability.â€

D. Robustness metrics (PromptBench)

Performance drop under each perturbation class

Worst-case performance across perturbations

Structural invariance: effect stability under paraphrase/reorder

E. Efficiency metrics

tokens in / tokens out

retries needed (validation loops)

cost per correct answer (if using paid APIs)

5) Experimental design (what you actually run)
A. Core design: multi-factor mixed model

For each task item 
ğ‘–
i, run:

prompt condition 
ğ‘
âˆˆ
{
ğ¶
0..
ğ¶
8
}
câˆˆ{C0..C8}

model 
ğ‘š
m across scale grid

seed / sampling run 
ğ‘Ÿ
=
1..
ğ‘…
r=1..R (R=5â€“20 depending on budget)

perturbation 
ğ‘
a from PromptBench (optional block)

Record metrics above.

This yields a dataset of runs:

{
(
ğ‘–
,
ğ‘
,
ğ‘š
,
ğ‘Ÿ
,
ğ‘
)
â†’
metrics
}
{(i,c,m,r,a)â†’metrics}
B. Trajectory sampling settings

You need at least two decoding regimes:

Greedy (baseline determinism)

Sampled (temperature, top-p), with fixed settings across conditions

For self-consistency conditions:

Sample K trajectories (e.g., K=10), aggregate by majority / verifier score.

C. Intervention tests (causal-style prompt edits)

To claim invariants, you need controlled counterfactuals:

Structure fixed, wording varied

Paraphrase the text but preserve operator tree

Wording fixed, structure altered

Same tokens mostly, but reorder constraints/schema/verifier blocks or change scope

These isolate structure vs surface form.

6) Mapping tools to implementation (wired to your matrix)
Prompt construction & constraints

LMQL / Guidance: encode operator bundles cleanly as programs and constraints

Outlines: enforce JSON schema or grammar at decoding time

Prompt optimization

DSPy: treat prompt/program as a parameterized policy; optimize under your objective (weighted metric)

Optional: compare to APE-style search for instruction strings

Robustness

PromptBench: apply perturbations systematically (and log which perturbation)

Multi-metric evaluation

HELM: run scenarios with consistent reporting (accuracy, robustness, calibration, etc.)

7) Statistical testing plan (this is where most papers are weakâ€”make yours strong)
A. Primary inference: hierarchical mixed-effects models

You have repeated measures per item and per model. Use a mixed model:

Fixed effects: prompt condition, model size, model family, decoding regime

Interactions: condition Ã— size, condition Ã— regime

Random effects: item id (and optionally domain)

Example outcome: exact match or hallucination rate.

Report:

estimated marginal means for each condition

interaction plots for scale dependence

B. Nonparametric paired tests for robustness

For each dataset/model pair:

paired bootstrap of per-item deltas (Ck â€“ C0)

permutation test on deltas (robust to non-normality)

Correct multiple comparisons with:

Holmâ€“Bonferroni or Benjaminiâ€“Hochberg (FDR)

C. Effect sizes

Always report:

Cohenâ€™s d (for continuous metrics)

odds ratios (for binary outcomes)

absolute risk reduction (hallucination metrics)

D. Distributional reporting (avoid cherry-picking)

For each condition, report:

mean Â± CI

median + IQR

worst-case over PromptBench perturbations

variance across seeds

8) Plotting plan (what figures your thesis will contain)

You want plots that communicate â€œcontrol over distributions,â€ not just average accuracy.

Figure set A â€” Core performance vs structure

Condition ladder plot: metric vs C0..C8, per model size

Interaction plot: condition effect vs model size (lines per condition)

Figure set B â€” Entropy / stability

Disagreement/entropy vs condition (sampled runs)

Variance decomposition: across seeds vs across prompts vs across items

Figure set C â€” Hallucination proxies

Unsupported claim rate vs condition (with/without retrieval)

Abstention confusion matrix (answerable vs unanswerable)

Figure set D â€” Robustness

PromptBench radar / bar: drop under each attack type

Worst-case robustness curve: min performance across perturbations

Figure set E â€” Efficiency tradeoffs

Pareto frontier: accuracy vs tokens (or hallucination vs tokens)

Cost per correct answer across conditions

9) â€œObjective functionâ€ for optimization experiments (DSPy)

To align with your info-theoretic framing, define a composite objective:

ğ½
(
ğ‘ƒ
)
=
ğ›¼
â‹…
TaskScore
â€…â€Š
âˆ’
â€…â€Š
ğ›½
â‹…
UnsupportedClaimRate
â€…â€Š
âˆ’
â€…â€Š
ğ›¾
â‹…
OutputEntropy
â€…â€Š
âˆ’
â€…â€Š
ğ›¿
â‹…
Cost
J(P)=Î±â‹…TaskScoreâˆ’Î²â‹…UnsupportedClaimRateâˆ’Î³â‹…OutputEntropyâˆ’Î´â‹…Cost

Where:

TaskScore: EM/F1/execution accuracy

UnsupportedClaimRate: UCR

OutputEntropy: seed variance or categorical entropy

Cost: tokens or $ proxy

DSPy can optimize prompt/program choices to maximize 
ğ½
J. Then you test whether DSPy-found structures resemble your operator bundles (thatâ€™s evidence for â€œoptimal structure has invariantsâ€).

10) What counts as a publishable result

You can claim something strong if you demonstrate:

Structure effects replicate across at least 3 domains and 3 model scales

Hard constraints reduce entropy and increase validity with quantified tradeoffs

Closed-loop reduces hallucination significantly more than open-loop on grounded tasks

A small set of operators gives most of the gains (invariant â€œbasisâ€)



â€¦



0) Experimental goal and core hypotheses
Goal

Model prompting as a control input that shapes the modelâ€™s trajectory distribution 
ğ‘
ğœƒ
(
ğœ
âˆ£
ğ‘ƒ
,
ğ‘¥
,
ğ‘‘
)
p
Î¸
	â€‹

(Ï„âˆ£P,x,d), and test whether structured prompts can be optimized to:

increase task-relevant information (proxy for higher 
ğ¼
(
ğ‘
;
ğ‘Œ
âˆ£
ğ‘‹
,
ğ‘ƒ
)
I(Z;Yâˆ£X,P))

decrease hallucination entropy / unsupported content (proxy for lower spurious uncertainty)

generalize across model scales/families

Hypotheses

H1 (Structure â†’ Information): Adding structure blocks (schema + constraints + verifier) increases task success and reduces output variance across seeds/prompts.

H2 (Support restriction â†’ Entropy): Hard constraints (CFG/schema constrained decoding) reduce â€œunsupported contentâ€ entropy more than post-hoc validation/retry loops at comparable token budgets.

H3 (Closed-loop â†’ Hallucination): ReAct-style feedback control yields larger hallucination reductions than open-loop prompting for knowledge-grounded tasks.

H4 (Scale invariants): Some operators have stable positive effect across model scales; others are scale-dependent (interaction effects).

1) Datasets and task suite (balanced for structure, truthfulness, and scale)

Pick a task suite that spans: structured output, reasoning, grounding, robustness, and domain variation.

A. Structured output tasks (constraint-friendly)

Information extraction: entity / relation extraction into JSON

Text-to-SQL / DSL generation: strict grammar

Classification with schema output: label + confidence + rationale field (optional)

Public datasets (good defaults):

Spider (Text-to-SQL) (structured; grammar constraints matter)

CoNLL03 / ACE-style NER (schema JSON output)

BoolQ / AG News / TREC (classification + schema)

B. Reasoning tasks (trajectory-sensitive)

GSM8K (math reasoning)

SVAMP (math word problems)

StrategyQA / CommonsenseQA (commonsense + brittle prompting)

C. Knowledge-grounded / hallucination-sensitive tasks

TruthfulQA (hallucination pressure)

FEVER (claim verification; can add retrieval / citations)

Natural Questions (short answer) (with and without retrieval)

D. Robustness / adversarial prompt perturbations

Use PromptBench attack suite to perturb prompts and test invariance under:

instruction injection

paraphrase / reorder

distractor context

role confusion

E. HELM scenarios (for multi-metric coverage)

Use HELM to ensure youâ€™re not optimizing just â€œaccuracyâ€ while hurting robustness or calibration.

Design note: you want at least 3 task archetypes per group, and at least 2 domains per archetype, to avoid â€œone dataset tells the whole storyâ€ syndrome.

2) Models and scale grid

You need a scale grid with at least:

small (~7B)

medium (~13â€“34B)

large (~70B or frontier API models)

For each family, keep decoding policy constant per condition, then test sensitivity.

Scale design:

Factor 1: model size (S/M/L)

Factor 2: model family (at least 2 families if possible)

Factor 3: decoding regime (greedy vs sampled)

Your analysis will explicitly model size Ã— operator interactions.

3) Prompt operator set (your â€œcontrol primitivesâ€)

Define prompts as compositions of operators on a base task spec 
ğ‘ƒ
0
P
0
	â€‹

.

Base template 
ğ‘ƒ
0
P
0
	â€‹


Minimal prompt: â€œDo task T on input X.â€

Operators (compose them factorially)

Open-loop structure operators

ROLE: assign identity + stance (e.g., â€œYou are a careful verifierâ€¦â€)

GOAL: explicit objective statement

CONSTRAINTS: must/should rules (length, style, allowed sources)

SCHEMA: explicit output format (JSON schema / EBNF)

EXEMPLARS: few-shot examples (k = 0, 1, 4)

DECOMPOSE: step plan or checklist (optionally hidden)

VERIFIER: self-check / consistency check / cite evidence lines

ABSTAIN: explicit â€œIf unknown, say â€˜INSUFFICIENT INFOâ€™â€

Closed-loop operators
9. REACT_LOOP: actâ†’observeâ†’revise (with tools or retrieval)
10. RETRY_ON_FAIL: validate; if invalid, revise and resubmit

Support restriction operators
11. CFG_CONSTRAIN: grammar constrained decoding (Outlines/Guidance)
12. REGEX_CONSTRAIN: regex constrained output
13. TYPE_CONSTRAIN: typed JSON schema enforcement

Canonical operator bundles (recommended conditions)

Instead of full 
2
13
2
13
 factorial (too big), test these bundles:

C0: Baseline 
ğ‘ƒ
0
P
0
	â€‹


C1: ROLE + GOAL

C2: + CONSTRAINTS

C3: + SCHEMA (no hard constraint yet)

C4: + VERIFIER + ABSTAIN

C5: + EXEMPLARS (few-shot)

C6: SCHEMA + CFG_CONSTRAIN (hard support restriction)

C7: REACT_LOOP (with/without retrieval)

C8: DSPy-optimized prompt/program (learned structure)

This gives you a ladder from heuristics â†’ formal constraint â†’ optimization.

4) Metrics (task success + information proxies + entropy/hallucination proxies)

You need three metric families:

A. Task performance

Accuracy / F1 / Exact Match (dataset-specific)

SQL execution accuracy (Spider)

JSON validity rate (structured tasks)

B. â€œTask-relevant mutual informationâ€ proxies (operational MI)

True 
ğ¼
(
ğ‘
;
ğ‘Œ
)
I(Z;Y) is hard because 
ğ‘
Z (latent task concept) isnâ€™t directly observed, so use proxies that correlate with â€œinformation about task-relevant factorsâ€:

Proxy 1: Minimal Description Length (MDL) / Compression score

Treat prompt as encoding of task rule; evaluate how well outputs predict labels with minimal extra bits.

Operationally: train a lightweight probe on outputs to predict ground truth; MDL decreases â†’ more task-relevant info.

Proxy 2: Contrastive separability of outputs

Given pairs 
(
ğ‘¥
,
ğ‘¦
âˆ—
)
(x,y
âˆ—
) and 
(
ğ‘¥
,
ğ‘¦
~
)
(x,
y
~
	â€‹

), score whether model outputs under prompt align closer to 
ğ‘¦
âˆ—
y
âˆ—
 than distractors.

Use an InfoNCE-like objective computed over outputs in embedding space.

Proxy 3: Calibration / confidence alignment (where available)

ECE (Expected Calibration Error) if you can get confidence; otherwise use self-reported confidence with care.

Better prompts should improve calibration on tasks with abstain option.

C. Hallucination / â€œunsupported entropyâ€ proxies

Pick auditable hallucination metrics:

Hallu Proxy 1: Unsupported claim rate (UCR)

Define â€œclaimsâ€ as extracted triples or named entities; mark unsupported if absent from provided context / retrieval docs.

Measure: unsupported claims per 100 tokens.

Hallu Proxy 2: Abstention correctness

On unanswerable items: does model abstain vs fabricate?

Hallu Proxy 3: Self-contradiction rate

Run verifier pass: consistency check across answer segments.

Hallu Proxy 4: Output entropy / variance across seeds

Sample N trajectories per prompt condition; compute:

answer disagreement rate

variance in key fields

entropy over categorical outputs

This connects directly to â€œtrajectory distribution stability.â€

D. Robustness metrics (PromptBench)

Performance drop under each perturbation class

Worst-case performance across perturbations

Structural invariance: effect stability under paraphrase/reorder

E. Efficiency metrics

tokens in / tokens out

retries needed (validation loops)

cost per correct answer (if using paid APIs)

5) Experimental design (what you actually run)
A. Core design: multi-factor mixed model

For each task item 
ğ‘–
i, run:

prompt condition 
ğ‘
âˆˆ
{
ğ¶
0..
ğ¶
8
}
câˆˆ{C0..C8}

model 
ğ‘š
m across scale grid

seed / sampling run 
ğ‘Ÿ
=
1..
ğ‘…
r=1..R (R=5â€“20 depending on budget)

perturbation 
ğ‘
a from PromptBench (optional block)

Record metrics above.

This yields a dataset of runs:

{
(
ğ‘–
,
ğ‘
,
ğ‘š
,
ğ‘Ÿ
,
ğ‘
)
â†’
metrics
}
{(i,c,m,r,a)â†’metrics}
B. Trajectory sampling settings

You need at least two decoding regimes:

Greedy (baseline determinism)

Sampled (temperature, top-p), with fixed settings across conditions

For self-consistency conditions:

Sample K trajectories (e.g., K=10), aggregate by majority / verifier score.

C. Intervention tests (causal-style prompt edits)

To claim invariants, you need controlled counterfactuals:

Structure fixed, wording varied

Paraphrase the text but preserve operator tree

Wording fixed, structure altered

Same tokens mostly, but reorder constraints/schema/verifier blocks or change scope

These isolate structure vs surface form.

6) Mapping tools to implementation (wired to your matrix)
Prompt construction & constraints

LMQL / Guidance: encode operator bundles cleanly as programs and constraints

Outlines: enforce JSON schema or grammar at decoding time

Prompt optimization

DSPy: treat prompt/program as a parameterized policy; optimize under your objective (weighted metric)

Optional: compare to APE-style search for instruction strings

Robustness

PromptBench: apply perturbations systematically (and log which perturbation)

Multi-metric evaluation

HELM: run scenarios with consistent reporting (accuracy, robustness, calibration, etc.)

7) Statistical testing plan (this is where most papers are weakâ€”make yours strong)
A. Primary inference: hierarchical mixed-effects models

You have repeated measures per item and per model. Use a mixed model:

Fixed effects: prompt condition, model size, model family, decoding regime

Interactions: condition Ã— size, condition Ã— regime

Random effects: item id (and optionally domain)

Example outcome: exact match or hallucination rate.

Report:

estimated marginal means for each condition

interaction plots for scale dependence

B. Nonparametric paired tests for robustness

For each dataset/model pair:

paired bootstrap of per-item deltas (Ck â€“ C0)

permutation test on deltas (robust to non-normality)

Correct multiple comparisons with:

Holmâ€“Bonferroni or Benjaminiâ€“Hochberg (FDR)

C. Effect sizes

Always report:

Cohenâ€™s d (for continuous metrics)

odds ratios (for binary outcomes)

absolute risk reduction (hallucination metrics)

D. Distributional reporting (avoid cherry-picking)

For each condition, report:

mean Â± CI

median + IQR

worst-case over PromptBench perturbations

variance across seeds

8) Plotting plan (what figures your thesis will contain)

You want plots that communicate â€œcontrol over distributions,â€ not just average accuracy.

Figure set A â€” Core performance vs structure

Condition ladder plot: metric vs C0..C8, per model size

Interaction plot: condition effect vs model size (lines per condition)

Figure set B â€” Entropy / stability

Disagreement/entropy vs condition (sampled runs)

Variance decomposition: across seeds vs across prompts vs across items

Figure set C â€” Hallucination proxies

Unsupported claim rate vs condition (with/without retrieval)

Abstention confusion matrix (answerable vs unanswerable)

Figure set D â€” Robustness

PromptBench radar / bar: drop under each attack type

Worst-case robustness curve: min performance across perturbations

Figure set E â€” Efficiency tradeoffs

Pareto frontier: accuracy vs tokens (or hallucination vs tokens)

Cost per correct answer across conditions

9) â€œObjective functionâ€ for optimization experiments (DSPy)

To align with your info-theoretic framing, define a composite objective:

ğ½
(
ğ‘ƒ
)
=
ğ›¼
â‹…
TaskScore
â€…â€Š
âˆ’
â€…â€Š
ğ›½
â‹…
UnsupportedClaimRate
â€…â€Š
âˆ’
â€…â€Š
ğ›¾
â‹…
OutputEntropy
â€…â€Š
âˆ’
â€…â€Š
ğ›¿
â‹…
Cost
J(P)=Î±â‹…TaskScoreâˆ’Î²â‹…UnsupportedClaimRateâˆ’Î³â‹…OutputEntropyâˆ’Î´â‹…Cost

Where:

TaskScore: EM/F1/execution accuracy

UnsupportedClaimRate: UCR

OutputEntropy: seed variance or categorical entropy

Cost: tokens or $ proxy

DSPy can optimize prompt/program choices to maximize 
ğ½
J. Then you test whether DSPy-found structures resemble your operator bundles (thatâ€™s evidence for â€œoptimal structure has invariantsâ€).

10) What counts as a publishable result

You can claim something strong if you demonstrate:

Structure effects replicate across at least 3 domains and 3 model scales

Hard constraints reduce entropy and increase validity with quantified tradeoffs

Closed-loop reduces hallucination significantly more than open-loop on grounded tasks

A small set of operators gives most of the gains (invariant â€œbasisâ€)

Confidence ratings (0â€“1)

Error free status: 0.91

Suitability for purpose: 0.95

Effectiveness: 0.93

Efficiency: 0.90

Completeness: 0.92

If you want to push all scores >0.95, the next refinement is to lock in a single canonical task suite (exact datasets + splits + scoring scripts) and specify exact seed counts / temperatures / K for self-consistencyâ€”I can do that in one shot based on your compute budget (local GPUs vs API).

do it fren

Alright â€” hereâ€™s a single canonical, fully specified experiment suite (datasets + exact splits + scoring + prompt conditions + decoding/seeds/K + statistical plan + plotting plan), with two compute profiles so you can run it on either local GPU(s) or API without changing the science.

Canonical Experiment Blueprint v1.0
1) Task suite (exact datasets, splits, metrics)

Youâ€™ll run 6 tasks covering: structured output, reasoning, and hallucination pressure. This is the smallest suite that still lets you claim cross-domain + cross-metric invariants.

T1 â€” Structured extraction â†’ strict JSON

Dataset: CoNLL-2003 NER (English)

Split: standard train/valid/test (use official split)

Task: Given a sentence, output entities as JSON list with fields: {"text","type","start","end"}

Primary metric: entity-level F1 (micro), from parsed JSON

Secondary metrics: JSON validity rate, field completeness, hallucinated entity rate (entities not present in text)

T2 â€” Classification â†’ schema output with abstain

Dataset: AG News

Split: standard train/test; carve 5k from train as validation (fixed random seed)

Task: output JSON: {"label": "...", "confidence": 0-1, "rationale": "â‰¤20 words"}
labels fixed to 4 classes

Primary metric: accuracy

Secondary: ECE (if you trust confidence), schema validity, abstain misuse rate (should be near zero)

T3 â€” Math reasoning (trajectory-sensitive)

Dataset: GSM8K

Split: standard test set (use official GSM8K test); optionally use 1k from train as dev

Task: final answer numeric; keep reasoning optional/controlled by condition

Primary metric: exact match on final answer

Secondary: variance across seeds, self-consistency gain

T4 â€” Commonsense reasoning (short answer)

Dataset: CommonsenseQA

Split: official dev set (for faster iteration) + test if available to you

Task: choose A/B/C/D/E; output JSON: {"choice":"A", "justification":"â‰¤25 words"}

Primary: accuracy

Secondary: robustness under prompt perturbations (PromptBench), choice entropy across seeds

T5 â€” Truthfulness / hallucination pressure (no retrieval)

Dataset: TruthfulQA (MC + generation)

Split: use full benchmark

Task: generate short answer + â€œI donâ€™t knowâ€ permitted; output JSON: {"answer":"...", "abstained": true/false}

Primary: TruthfulQA truthfulness metric if using standard eval; otherwise use provided â€œtruthfulness/ informativenessâ€ scorers

Secondary: abstention correctness (when appropriate), contradiction rate

T6 â€” Claim verification with evidence (closed-loop benefit)

Dataset: FEVER

Split: dev set (initially) + test later

Task: output JSON: {"label":"SUPPORTS|REFUTES|NOT_ENOUGH_INFO","evidence":[...]}

Primary: label accuracy

Secondary: evidence precision/recall; unsupported evidence rate

Retrieval: for closed-loop conditions only (simple BM25 over Wikipedia dump or an API retriever if you have one)

This suite is deliberately half structured and half reasoning/robustness/hallucination so â€œentropy reductionâ€ isnâ€™t conflated with â€œformat compliance.â€

2) Prompt conditions (fixed operator bundles)

Youâ€™ll use the same base prompt scaffold across tasks with task-specific slots.

C0 Baseline: minimal instruction
C1 Role+Goal: add role + explicit goal
C2 +Constraints: â€œmust/shouldâ€ rules, abstain rule
C3 +Schema: explicit JSON schema (no hard decode constraint)
C4 +Verifier: self-check checklist + â€œrevise once if invalidâ€
C5 +Few-shot: 2 exemplars (fixed)
C6 Hard constraint: schema + constrained decoding (Outlines/Guidance)
C7 Closed loop: ReAct-style (only on FEVER; optionally TruthfulQA with retrieval off/on)
C8 Optimized: DSPy-compiled prompt/program optimizing objective 
ğ½
J

Important control rule: For each condition, keep total prompt token length within a band:

structured tasks: target 350â€“650 prompt tokens

reasoning tasks: 200â€“500 prompt tokens
This avoids â€œlonger prompt winsâ€ confounds.

3) Models + scale grid (explicit)

Use 3 sizes Ã— 2 families if possible:

Small: ~7â€“8B instruct

Medium: ~13â€“34B instruct

Large: ~70B+ instruct (or frontier API model)

If you can only do one family, do size scaling within it and add an API frontier model as â€œfamily Bâ€.

4) Decoding + sampling plan (exact temperatures, seeds, K)

Youâ€™ll run two decoding regimes for every (task, model, condition):

Regime D0: Deterministic

temperature = 0.0

top_p = 1.0

max_tokens: set per task (see below)

Regime D1: Sampled

temperature = 0.7

top_p = 0.95

repetition penalty: off (unless your stack forces it)

seeds: fixed list

Seeds and replicates

R = 10 independent samples per item for D1 (seeded)

R = 1 for D0

This lets you measure trajectory entropy / variance without exploding cost.

Self-consistency (only for GSM8K and CommonsenseQA)

In conditions where self-consistency is enabled (C4, C5, C8 optionally):

sample K = 10 trajectories (reuse D1 samples)

aggregate by majority vote on final answer / choice

tie-break by verifier score (if available)

Task max_tokens (fixed)

CoNLL JSON: 220

AG News JSON: 120

GSM8K: 320

CommonsenseQA JSON: 140

TruthfulQA: 180

FEVER JSON: 260 (excluding retrieved evidence chunking)

5) Constrained decoding specifics (C6)

For C6 you must enforce constraints at decode time:

CoNLL / AG / CommonsenseQA / TruthfulQA / FEVER: JSON schema constraint

Spider (if you later add it): CFG/EBNF SQL grammar constraint

Implementation:

Prefer Outlines for JSON schema constraints (fast, clean)

Use Guidance if you need CFG/regex hybrid

Metrics to log specifically for C6:

constraint failure rate (should be ~0)

token overhead vs unconstrained

effect on task accuracy (sometimes constraints reduce semantic quality)

6) Hallucination / â€œunsupported contentâ€ measurement (fully specified)
A) Structured tasks (CoNLL, FEVER)

Unsupported entity rate (CoNLL): predicted entity spans not substrings of input

Unsupported evidence rate (FEVER): evidence sentences not in retrieved set / not matching wiki ids

B) TruthfulQA

Abstention correctness: if question is unanswerable / unknown â†’ abstain

Contradiction rate: run a verifier prompt to check internal consistency (same model, temperature 0.0)

C) Variance / entropy

For classification-like outputs (AG, CommonsenseQA, FEVER label):

compute categorical entropy across the R=10 samples

For generation:

compute disagreement rate on extracted key facts (simple IE pass: entities + numbers)

7) PromptBench robustness block (exact)

Run PromptBench perturbations on two tasks only (to control cost):

CommonsenseQA (reasoning robustness)

CoNLL JSON extraction (format + semantic robustness)

Perturbation set:

instruction injection

paraphrase prompt

reorder blocks

add distractor context

role confusion

Protocol:

Apply perturbation to C2, C3, C4, C6 (not all conditions)

Measure worst-case drop and variance change

8) DSPy optimization experiment (C8) â€“ exact objective and budget
Objective 
ğ½
J
ğ½
=
1.0
â‹…
TaskScore
âˆ’
0.7
â‹…
UnsupportedRate
âˆ’
0.3
â‹…
Entropy
âˆ’
0.1
â‹…
TokenCost
J=1.0â‹…TaskScoreâˆ’0.7â‹…UnsupportedRateâˆ’0.3â‹…Entropyâˆ’0.1â‹…TokenCost
Optimization budget

Use dev sets only:

CoNLL dev: 500 items

AG dev: 500 items

GSM8K dev: 300 items

FEVER dev: 300 items

Iterations: 40 candidate prompts/program variants per task-model pair (start)

Select best structure then evaluate on test with frozen prompt

This avoids test leakage and keeps optimization replicable.

9) Statistical tests (exact)
Primary: hierarchical mixed-effects models

For each metric 
ğ‘€
M, fit:

Fixed effects: condition, model_size, family, decoding_regime

Interactions: conditionÃ—size, conditionÃ—regime

Random intercept: item_id

Random intercept: dataset (if you pool tasks)

Report:

effect sizes per condition vs C0

interaction plots (does operator benefit grow/shrink with size?)

Secondary: paired bootstrap deltas (robust, simple)

For each dataset-model pair:

compute per-item delta: 
Î”
ğ‘–
=
ğ‘€
ğ‘–
(
ğ¶
ğ‘˜
)
âˆ’
ğ‘€
ğ‘–
(
ğ¶
0
)
Î”
i
	â€‹

=M
i
	â€‹

(Ck)âˆ’M
i
	â€‹

(C0)

bootstrap items 10,000 resamples â†’ 95% CI

significance by whether CI crosses 0

Multiple comparisons

Use Benjaminiâ€“Hochberg FDR across conditions per dataset

Robustness block

Use â€œmin-over-attacksâ€ metric and test deltas with paired bootstrap

10) Plotting plan (exact figures)

Youâ€™ll produce these figures consistently for every dataset:

Core performance

Condition ladder (C0..C8) with 95% CI, separate lines for size (S/M/L)

Entropy / variance

Output entropy vs condition (D1 sampled regime)

Variance decomposition bar: within-seed vs across-seed vs across-perturbation

Hallucination proxies

Unsupported rate vs condition (with retrieval off/on for FEVER)

Abstain confusion matrix for TruthfulQA

Robustness

Worst-case drop under attacks for selected conditions (C2/C3/C4/C6)

Efficiency

Pareto plot: TaskScore vs TokenCost (points are conditions, per model size)

11) Compute profiles (choose one; both are valid)
Profile A â€” â€œAPI / modest budgetâ€

Items per dataset: 500 test items (sampled uniformly, fixed seed)

R=10 for sampled regime on only 2 datasets (GSM8K + TruthfulQA)

Others use R=3

Total runs manageable; still supports invariance claims

Profile B â€” â€œFull thesis-gradeâ€

CoNLL test: full

AG test: 7,600 (full)

GSM8K test: full

CommonsenseQA dev: full

TruthfulQA: full

FEVER dev: 5,000 items

R=10 everywhere in D1

If you donâ€™t tell me your budget, I recommend Profile A for iteration + Profile B for the final thesis runs.

12) Deliverables (what youâ€™ll have at the end)

A single run table with columns: (task, item_id, model, size, family, condition, decoding, seed, perturbation, metrics...)

Plots + effect size tables demonstrating:

which structural operators increase â€œtask-relevant signalâ€

which reduce hallucination proxies/entropy

which are invariant across scales/families



...


1) Repo layout
prompt-control-suite/
  pyproject.toml
  README.md
  configs/
    profile_A.yaml
    profile_B.yaml
    operators_v1.yaml
    tasks_v1.yaml
    models_v1.yaml
    decoding_v1.yaml
    metrics_v1.yaml
  src/
    pcsuite/
      __init__.py
      cli.py
      config.py
      registry.py

      runners/
        run_suite.py
        run_task.py
        run_helm.py
        run_promptbench.py

      data/
        datasets.py
        splits.py
        cache.py

      prompts/
        base_templates/
          json_base.j2
          reasoning_base.j2
        operators/
          operators.py
          render.py
          validate.py
        conditions/
          conditions.py

      engines/
        model_api.py
        adapters/
          base.py
          openai_api.py
          local_hf.py

      constraints/
        outlines_json.py
        guidance_cfg.py
        lmql_program.py

      optimization/
        dspy_opt.py
        ape_opt.py

      evaluation/
        metrics.py
        extractors.py
        truthfulness.py
        fever.py
        calibration.py
        robustness.py

      perturbations/
        promptbench_wrap.py
        perturbations.py

      logging/
        schema.py
        writer.py
        aggregate.py
        plots.py

      utils/
        seeds.py
        hashing.py
        timing.py

  scripts/
    download_datasets.sh
    build_wiki_index.py
    run_profile_A.sh
    run_profile_B.sh

  outputs/
    runs/        # raw jsonl
    tables/      # parquet/csv
    plots/       # figures
    reports/     # markdown/html summaries

Design principle: everything is driven by YAML; code is mostly adapters + evaluation. The runner never â€œspecial casesâ€ a task/model â€” it dispatches via registries.

2) Config schema (YAML)

Youâ€™ll have a top-level profile config that references versioned task/model/operator definitions.

2.1 Profile config schema (configs/profile_*.yaml)
version: "1.0"

run:
  name: "profile_A_2026-02-22"
  output_dir: "outputs"
  overwrite: false
  cache_dir: ".cache"
  log_level: "INFO"

repro:
  global_seed: 1337
  deterministic_hash_seed: "pcsuite-v1"
  seeds:
    d0_greedy: [0]              # deterministic regime
    d1_sampled: [1,2,3,4,5,6,7,8,9,10]  # R=10
  num_workers: 8                # for dataset preproc + evaluation
  max_inflight: 16              # async API concurrency (if used)

suite:
  tasks_ref: "configs/tasks_v1.yaml"
  models_ref: "configs/models_v1.yaml"
  operators_ref: "configs/operators_v1.yaml"
  decoding_ref: "configs/decoding_v1.yaml"
  metrics_ref: "configs/metrics_v1.yaml"

selection:
  tasks: ["conll03_ner_json", "agnews_json", "gsm8k", "csqa_json", "truthfulqa_gen", "fever_json"]
  models: ["small_local", "medium_local", "large_api"]   # must exist in models_v1
  conditions: ["C0","C1","C2","C3","C4","C5","C6","C7","C8"]
  decoding_regimes: ["D0","D1"]

limits:
  max_items_per_task: 500        # Profile A sampling; set null for full
  max_tokens_overall: null       # optional hard budget
  stop_on_error_rate: 0.02       # if >2% runs error, abort

integrations:
  helm:
    enabled: true
    scenarios: ["mmlu", "truthfulqa"]     # optional; HELM scenario names
    helm_output_dir: "outputs/helm"
  promptbench:
    enabled: true
    tasks: ["csqa_json", "conll03_ner_json"]
    conditions_subset: ["C2","C3","C4","C6"]
    attack_types: ["injection","paraphrase","reorder","distractor","role_confusion"]
    max_items: 300
  retrieval:
    enabled_for_tasks: ["fever_json"]
    backend: "bm25_local"        # bm25_local | api_retriever
    wiki_index_path: ".cache/wiki_bm25"

optimization:
  dspy:
    enabled: true
    conditions_target: ["C8"]
    dev_items:
      conll03_ner_json: 500
      agnews_json: 500
      gsm8k: 300
      fever_json: 300
    iterations: 40
    objective:
      task_weight: 1.0
      unsupported_weight: 0.7
      entropy_weight: 0.3
      token_cost_weight: 0.1
3) Unified logging + artifact formats

Youâ€™ll log every single model call and every evaluation in a unified, audit-friendly way.

3.1 Raw run log: JSONL (outputs/runs/<run_name>.jsonl)

One row per (task,item,model,condition,decoding,seed,perturbation).

{
  "run_id": "8b6d...sha256",
  "timestamp_utc": "2026-02-22T03:14:15Z",
  "profile": "profile_A_2026-02-22",

  "task": {"id":"gsm8k","split":"test","item_id":"GSM8K:test:01234"},
  "model": {"id":"medium_local","family":"llama","size":"13b","provider":"hf"},
  "condition": {"id":"C4","operators":["ROLE","GOAL","CONSTRAINTS","SCHEMA","VERIFIER","ABSTAIN"]},
  "decoding": {"id":"D1","temperature":0.7,"top_p":0.95,"max_tokens":320},
  "seed": 7,
  "perturbation": {"type": null, "id": null},

  "prompt": {
    "prompt_hash":"aa12..",
    "template":"reasoning_base.j2",
    "rendered_chars": 1820
  },

  "generation": {
    "text":"...",
    "finish_reason":"stop",
    "latency_ms": 1240,
    "usage": {"prompt_tokens": 410, "completion_tokens": 215, "total_tokens": 625}
  },

  "parsed": {
    "json_valid": null,
    "fields": {"final_answer":"42"},
    "parse_errors":[]
  },

  "metrics": {
    "task_score": 1.0,
    "exact_match": 1.0,
    "entropy_cat": null,
    "unsupported_rate": 0.0,
    "abstain_correct": null
  },

  "errors": []
}
3.2 Aggregated tables

outputs/tables/runs.parquet (all rows)

outputs/tables/summary_by_condition.csv (means + CI per condition/model/task)

outputs/tables/robustness_worst_case.csv (PromptBench min-over-attacks)

3.3 Plots

Saved to outputs/plots/ with deterministic filenames:

perf_condition_ladder__<task>__<model>.png

entropy_vs_condition__<task>__<model>.png

pareto_accuracy_tokens__<task>__grid.png

robustness_drop__<task>__grid.png

4) Adapter interfaces (how integrations plug in)

Everything is built around 5 registries:

TaskRegistry: yields items and ground truth

ConditionRegistry: composes operator bundles into a prompt program

ModelAdapterRegistry: runs generation

ConstraintRegistry: optional constrained decoding

MetricRegistry: parses output â†’ metrics

4.1 ModelAdapter interface (src/pcsuite/engines/adapters/base.py)
class ModelAdapter:
    id: str
    def generate(self, prompt: str, decoding: dict, seed: int) -> dict:
        """
        Returns:
          {"text": str, "finish_reason": str, "latency_ms": int,
           "usage": {"prompt_tokens": int, "completion_tokens": int, "total_tokens": int}}
        """

Adapters:

openai_api.py (API)

local_hf.py (Transformers / vLLM)

(Optional) ollama.py if you want fast local demos

4.2 Constraint interface

Constraints are invoked only for C6 (and optionally others):

class ConstraintEngine:
    id: str
    def generate_constrained(self, adapter: ModelAdapter, prompt: str, decoding: dict, schema_or_grammar: dict, seed: int) -> dict:
        """Same output shape as adapter.generate."""

Engines:

outlines_json.py (JSON schema)

guidance_cfg.py (CFG/regex)

lmql_program.py (LMQL query compiled/run)

4.3 Condition interface

Each condition yields:

rendered prompt text

meta: operators used, schema, verifier config, etc.

class Condition:
    id: str
    def build(self, task_spec: dict, item: dict) -> dict:
        """
        Returns:
          {"prompt": str, "meta": {...}, "schema": optional, "verifier": optional}
        """
4.4 Parsing + metrics interface

Parse is task-specific; metrics are common.

class TaskEvaluator:
    task_id: str
    def parse(self, text: str) -> dict
    def score(self, parsed: dict, truth: dict, extra: dict) -> dict
5) Versioned definitions (tasks/models/operators/decoding/metrics)

These are the â€œfrozenâ€ parts that keep runs comparable.

5.1 Tasks (configs/tasks_v1.yaml)
version: "1.0"
tasks:
  conll03_ner_json:
    kind: "structured_extraction"
    dataset: "conll2003"
    split: {train: "train", dev: "validation", test: "test"}
    input_fields: ["tokens"]
    truth_fields: ["spans"]
    output_schema_ref: "schemas/conll03_ner.json"
    max_tokens: 220

  agnews_json:
    kind: "classification"
    dataset: "ag_news"
    split: {train: "train", dev: "dev_5k", test: "test"}
    labels: ["World","Sports","Business","Sci/Tech"]
    output_schema_ref: "schemas/agnews.json"
    max_tokens: 120

  gsm8k:
    kind: "reasoning"
    dataset: "gsm8k"
    split: {dev: "train_1k", test: "test"}
    answer_extractor: "gsm8k_numeric"
    max_tokens: 320

  csqa_json:
    kind: "mcq"
    dataset: "commonsense_qa"
    split: {dev: "dev"}
    output_schema_ref: "schemas/csqa.json"
    max_tokens: 140

  truthfulqa_gen:
    kind: "truthfulness"
    dataset: "truthful_qa"
    split: {test: "validation"}   # common naming in loaders; normalize in code
    output_schema_ref: "schemas/truthfulqa.json"
    max_tokens: 180

  fever_json:
    kind: "verification"
    dataset: "fever"
    split: {dev: "dev"}
    retrieval: {enabled: true}
    output_schema_ref: "schemas/fever.json"
    max_tokens: 260
5.2 Models (configs/models_v1.yaml)
version: "1.0"
models:
  small_local:
    provider: "hf"
    family: "llama"
    size: "7b"
    model_name_or_path: "meta-llama/Llama-2-7b-chat-hf"
    device: "cuda"
    dtype: "bfloat16"

  medium_local:
    provider: "hf"
    family: "llama"
    size: "13b"
    model_name_or_path: "meta-llama/Llama-2-13b-chat-hf"
    device: "cuda"
    dtype: "bfloat16"

  large_api:
    provider: "openai"
    family: "frontier"
    size: "api"
    model: "gpt-4.1"   # example placeholder; set to what you actually use
5.3 Decoding (configs/decoding_v1.yaml)
version: "1.0"
decoding:
  D0:
    temperature: 0.0
    top_p: 1.0
  D1:
    temperature: 0.7
    top_p: 0.95
5.4 Operators (configs/operators_v1.yaml)
version: "1.0"
operators:
  ROLE: {type: "prefix_block", template: "You are a careful, literal verifier.\n"}
  GOAL: {type: "prefix_block", template: "Your goal is: {{task_goal}}\n"}
  CONSTRAINTS: {type: "block", template: "Rules:\n- {{rules | join('\n- ')}}\n"}
  SCHEMA: {type: "block", template: "Output must be valid JSON matching this schema:\n{{schema_text}}\n"}
  EXEMPLARS: {type: "fewshot", k: 2}
  VERIFIER: {type: "post_block", template: "Before finalizing, check: {{checklist | join(', ')}}.\n"}
  ABSTAIN: {type: "rule", template: "If you are unsure or lack evidence, output abstained=true.\n"}
  REACT_LOOP: {type: "program", engine: "react"}
  CFG_CONSTRAIN: {type: "constraint", engine: "guidance_cfg"}
  JSON_CONSTRAIN: {type: "constraint", engine: "outlines_json"}
5.5 Metrics (configs/metrics_v1.yaml)
version: "1.0"
metrics:
  task_score: {type: "primary"}
  json_valid: {type: "format"}
  unsupported_rate: {type: "hallucination"}
  entropy_cat: {type: "entropy"}
  token_cost: {type: "efficiency"}
6) Example profiles (A and B)
6.1 Profile A (iteration / modest budget) â€” configs/profile_A.yaml

Key settings:

max_items_per_task: 500

D1 seeds 10 only for GSM8K + TruthfulQA; others use 3 seeds (optional knob)

# ...same schema as earlier...
limits:
  max_items_per_task: 500

selection:
  decoding_regimes: ["D0","D1"]

repro:
  seeds:
    d0_greedy: [0]
    d1_sampled: [1,2,3,4,5,6,7,8,9,10]
6.2 Profile B (final thesis-grade) â€” configs/profile_B.yaml
limits:
  max_items_per_task: null

repro:
  seeds:
    d0_greedy: [0]
    d1_sampled: [1,2,3,4,5,6,7,8,9,10]
7) CLI entrypoints (canonical commands)

Youâ€™ll have a single CLI with subcommands:

# 1) Run the full suite
python -m pcsuite.cli run --config configs/profile_A.yaml

# 2) Run just one task / one model (debug)
python -m pcsuite.cli run-task --config configs/profile_A.yaml --task gsm8k --model medium_local

# 3) Run PromptBench robustness block
python -m pcsuite.cli promptbench --config configs/profile_A.yaml

# 4) Run HELM scenarios (optional)
python -m pcsuite.cli helm --config configs/profile_A.yaml

# 5) Aggregate + plots
python -m pcsuite.cli report --config configs/profile_A.yaml
8) Minimal runner behavior (what â€œrunâ€ must do)

For each selected task:

Load dataset + split

Sample max_items_per_task deterministically (seeded)

For each model Ã— condition Ã— decoding Ã— seed:

build prompt via Condition.build(...)

choose generation engine:

if condition includes hard constraint â†’ ConstraintEngine.generate_constrained(...)

else â†’ ModelAdapter.generate(...)

parse output

compute metrics

write JSONL row

Aggregate + compute CIs + plots

Hard rule: the run is append-only and idempotent by run_id hash, so you can resume safely.

9) Where HELM fits cleanly

HELM can be run as:

an independent evaluation block to cross-check your custom harness

a â€œscenario runnerâ€ that outputs to outputs/helm, then you import summary metrics into your report

Your runner should treat HELM outputs as external evidence, not as the primary run log (keeps your suite self-contained).

10) Final implementation guardrails (so results are publishable)

Freeze configs by git tag per run (run.name includes tag)

Write prompt_hash and schema_hash per run row

Log raw prompts (optionally compressed) for audit

Always report distributions (seed variance, prompt perturbations)






...





appendix ? some files


...



1) json_base.j2

Path: src/pcsuite/prompts/base_templates/json_base.j2

{# pcsuite JSON-base template (open-loop) #}
{# Inputs expected in context:
   task_title, task_goal, input_text, rules (list[str]),
   schema_text (str), exemplars_text (str|None),
   verifier_checklist (list[str]|None),
   allow_abstain (bool), abstain_instruction (str|None)
#}

{% if role_block -%}
{{ role_block }}
{% endif -%}

{% if goal_block -%}
{{ goal_block }}
{% else -%}
Task: {{ task_title }}
Goal: {{ task_goal }}
{% endif -%}

{% if context_block -%}
{{ context_block }}
{% endif -%}

{% if rules_block -%}
{{ rules_block }}
{% elif rules and rules|length > 0 -%}
Rules:
{% for r in rules -%}
- {{ r }}
{% endfor -%}
{% endif -%}

{% if allow_abstain and abstain_instruction -%}
{{ abstain_instruction }}
{% endif -%}

{% if exemplars_text -%}
Examples:
{{ exemplars_text }}
{% endif -%}

{% if schema_text -%}
Output must be valid JSON matching this schema (no extra keys):
{{ schema_text }}
{% endif -%}

Input:
{{ input_text }}

{% if verifier_block -%}
{{ verifier_block }}
{% endif -%}

Return ONLY JSON.
2) reasoning_base.j2

Path: src/pcsuite/prompts/base_templates/reasoning_base.j2

{# pcsuite reasoning-base template (open-loop) #}
{# Inputs expected in context:
   task_title, task_goal, input_text,
   rules (list[str]), allow_abstain (bool), abstain_instruction (str|None),
   reasoning_mode ("none"|"brief"|"full"),  # controlled by condition
   final_answer_format (str|None),
   verifier_checklist (list[str]|None),
   exemplars_text (str|None)
#}

{% if role_block -%}
{{ role_block }}
{% endif -%}

{% if goal_block -%}
{{ goal_block }}
{% else -%}
Task: {{ task_title }}
Goal: {{ task_goal }}
{% endif -%}

{% if rules and rules|length > 0 -%}
Rules:
{% for r in rules -%}
- {{ r }}
{% endfor -%}
{% endif -%}

{% if allow_abstain and abstain_instruction -%}
{{ abstain_instruction }}
{% endif -%}

{% if exemplars_text -%}
Examples:
{{ exemplars_text }}
{% endif -%}

Input:
{{ input_text }}

{% if reasoning_mode == "full" -%}
Work step-by-step. Then provide the final answer.
{% elif reasoning_mode == "brief" -%}
Think briefly. Then provide the final answer.
{% else -%}
Provide the final answer directly.
{% endif -%}

{% if final_answer_format -%}
Final answer format: {{ final_answer_format }}
{% endif -%}

{% if verifier_block -%}
{{ verifier_block }}
{% endif -%}

Return the final answer only.
3) Operators: definitions + rendering
3.1 operators.py

Path: src/pcsuite/prompts/operators/operators.py

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional


@dataclass(frozen=True)
class OperatorOutput:
    # Named blocks for templates
    role_block: Optional[str] = None
    goal_block: Optional[str] = None
    context_block: Optional[str] = None
    rules_block: Optional[str] = None
    verifier_block: Optional[str] = None
    exemplars_text: Optional[str] = None
    # Flags/extra
    allow_abstain: bool = False
    abstain_instruction: Optional[str] = None
    reasoning_mode: Optional[str] = None  # "none"|"brief"|"full"


class Operator:
    """
    Operators mutate/extend an OperatorOutput deterministically given task+item context.
    """
    id: str

    def apply(self, ctx: Dict[str, Any], out: OperatorOutput) -> OperatorOutput:
        raise NotImplementedError


class RoleOperator(Operator):
    id = "ROLE"

    def __init__(self, text: str):
        self.text = text.strip() + "\n"

    def apply(self, ctx: Dict[str, Any], out: OperatorOutput) -> OperatorOutput:
        return OperatorOutput(**{**out.__dict__, "role_block": self.text})


class GoalOperator(Operator):
    id = "GOAL"

    def __init__(self, template: str):
        self.template = template

    def apply(self, ctx: Dict[str, Any], out: OperatorOutput) -> OperatorOutput:
        task_goal = ctx.get("task_goal", "")
        task_title = ctx.get("task_title", "")
        text = self.template.format(task_goal=task_goal, task_title=task_title).rstrip() + "\n"
        return OperatorOutput(**{**out.__dict__, "goal_block": text})


class ConstraintsOperator(Operator):
    id = "CONSTRAINTS"

    def __init__(self, heading: str = "Rules:"):
        self.heading = heading

    def apply(self, ctx: Dict[str, Any], out: OperatorOutput) -> OperatorOutput:
        rules: List[str] = ctx.get("rules", []) or []
        if not rules:
            return out
        lines = [self.heading] + [f"- {r}" for r in rules]
        text = "\n".join(lines).rstrip() + "\n"
        return OperatorOutput(**{**out.__dict__, "rules_block": text})


class AbstainOperator(Operator):
    id = "ABSTAIN"

    def __init__(self, instruction: str):
        self.instruction = instruction.strip()

    def apply(self, ctx: Dict[str, Any], out: OperatorOutput) -> OperatorOutput:
        return OperatorOutput(
            **{
                **out.__dict__,
                "allow_abstain": True,
                "abstain_instruction": self.instruction + "\n",
            }
        )


class ExemplarsOperator(Operator):
    id = "EXEMPLARS"

    def __init__(self, k: int = 2):
        self.k = k

    def apply(self, ctx: Dict[str, Any], out: OperatorOutput) -> OperatorOutput:
        # Expect ctx["exemplars"] as list of {"input":..., "output":...}
        exemplars = (ctx.get("exemplars") or [])[: self.k]
        if not exemplars:
            return out
        chunks = []
        for ex in exemplars:
            chunks.append("Input:\n" + str(ex["input"]).rstrip())
            chunks.append("Output:\n" + str(ex["output"]).rstrip())
            chunks.append("")  # blank line
        text = "\n".join(chunks).rstrip() + "\n"
        return OperatorOutput(**{**out.__dict__, "exemplars_text": text})


class VerifierOperator(Operator):
    id = "VERIFIER"

    def __init__(self, checklist: Optional[List[str]] = None, one_revision: bool = True):
        self.checklist = checklist or [
            "JSON is valid (if required)",
            "All constraints are satisfied",
            "No invented facts or entities",
        ]
        self.one_revision = one_revision

    def apply(self, ctx: Dict[str, Any], out: OperatorOutput) -> OperatorOutput:
        checklist = ctx.get("verifier_checklist") or self.checklist
        header = "Before finalizing:"
        lines = [header] + [f"- Check: {c}" for c in checklist]
        if self.one_revision:
            lines.append("- If any check fails, revise once, then output the corrected final answer.")
        text = "\n".join(lines).rstrip() + "\n"
        return OperatorOutput(**{**out.__dict__, "verifier_block": text})


class ReasoningModeOperator(Operator):
    id = "REASONING_MODE"

    def __init__(self, mode: str):
        if mode not in ("none", "brief", "full"):
            raise ValueError("reasoning mode must be: none|brief|full")
        self.mode = mode

    def apply(self, ctx: Dict[str, Any], out: OperatorOutput) -> OperatorOutput:
        return OperatorOutput(**{**out.__dict__, "reasoning_mode": self.mode})
3.2 render.py (compose operators + render Jinja)

Path: src/pcsuite/prompts/operators/render.py

from __future__ import annotations

from dataclasses import asdict
from typing import Any, Dict, List, Tuple

from jinja2 import Environment, FileSystemLoader, StrictUndefined

from .operators import Operator, OperatorOutput


def make_jinja_env(templates_dir: str) -> Environment:
    return Environment(
        loader=FileSystemLoader(templates_dir),
        undefined=StrictUndefined,  # fail fast on missing fields
        autoescape=False,
        trim_blocks=True,
        lstrip_blocks=True,
    )


def apply_operators(ctx: Dict[str, Any], ops: List[Operator]) -> OperatorOutput:
    out = OperatorOutput()
    for op in ops:
        out = op.apply(ctx, out)
    return out


def render_prompt(
    env: Environment,
    template_name: str,
    ctx: Dict[str, Any],
    ops: List[Operator],
) -> Tuple[str, Dict[str, Any]]:
    op_out = apply_operators(ctx, ops)
    merged = {**ctx, **asdict(op_out)}
    text = env.get_template(template_name).render(**merged).strip() + "\n"
    meta = {
        "template": template_name,
        "operators": [getattr(o, "id", o.__class__.__name__) for o in ops],
        "allow_abstain": op_out.allow_abstain,
        "reasoning_mode": op_out.reasoning_mode,
    }
    return text, meta
4) Conditions C0â€“C8 (executable pipelines)
4.1 conditions.py

Path: src/pcsuite/prompts/conditions/conditions.py

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional

from pcsuite.prompts.operators.operators import (
    AbstainOperator,
    ConstraintsOperator,
    ExemplarsOperator,
    GoalOperator,
    Operator,
    ReasoningModeOperator,
    RoleOperator,
    VerifierOperator,
)
from pcsuite.prompts.operators.render import make_jinja_env, render_prompt


@dataclass(frozen=True)
class BuiltPrompt:
    prompt: str
    meta: Dict[str, Any]
    # Optional constraint payload (for C6)
    constraint: Optional[Dict[str, Any]] = None
    # Optional program flags (for C7)
    react: Optional[Dict[str, Any]] = None


class Condition:
    id: str

    def build(self, *, templates_dir: str, ctx: Dict[str, Any]) -> BuiltPrompt:
        raise NotImplementedError


def _template_for_task(ctx: Dict[str, Any]) -> str:
    # Task kind decides template
    kind = ctx.get("task_kind")
    if kind in ("structured_extraction", "classification", "mcq", "verification", "truthfulness"):
        return "json_base.j2"
    return "reasoning_base.j2"


def _default_goal_template() -> str:
    return "Goal: {task_goal}\n"


def _default_role() -> str:
    return "You are a careful, literal assistant. Follow instructions exactly."


def _default_abstain_instruction() -> str:
    return "If you are unsure or lack evidence, set abstained=true and answer=\"INSUFFICIENT_INFO\"."


def _ensure_defaults(ctx: Dict[str, Any]) -> Dict[str, Any]:
    # Non-destructive defaults
    out = dict(ctx)
    out.setdefault("task_title", out.get("task_id", "Task"))
    out.setdefault("task_goal", "Complete the task correctly.")
    out.setdefault("rules", [])
    out.setdefault("schema_text", out.get("schema_text", ""))
    out.setdefault("exemplars", out.get("exemplars", []))
    out.setdefault("verifier_checklist", out.get("verifier_checklist", []))
    out.setdefault("final_answer_format", out.get("final_answer_format", None))
    out.setdefault("input_text", out.get("input_text", ""))
    out.setdefault("task_kind", out.get("task_kind", "reasoning"))
    return out


# ----------------------
# C0..C8 conditions
# ----------------------

class C0_Baseline(Condition):
    id = "C0"

    def build(self, *, templates_dir: str, ctx: Dict[str, Any]) -> BuiltPrompt:
        ctx = _ensure_defaults(ctx)
        env = make_jinja_env(templates_dir)
        template = _template_for_task(ctx)
        prompt, meta = render_prompt(env, template, ctx, ops=[])
        return BuiltPrompt(prompt=prompt, meta=meta)


class C1_RoleGoal(Condition):
    id = "C1"

    def build(self, *, templates_dir: str, ctx: Dict[str, Any]) -> BuiltPrompt:
        ctx = _ensure_defaults(ctx)
        env = make_jinja_env(templates_dir)
        template = _template_for_task(ctx)
        ops: List[Operator] = [
            RoleOperator(_default_role()),
            GoalOperator(_default_goal_template()),
        ]
        prompt, meta = render_prompt(env, template, ctx, ops=ops)
        return BuiltPrompt(prompt=prompt, meta=meta)


class C2_AddConstraints(Condition):
    id = "C2"

    def build(self, *, templates_dir: str, ctx: Dict[str, Any]) -> BuiltPrompt:
        ctx = _ensure_defaults(ctx)
        # Ensure a couple universal constraints to reduce spurious entropy
        rules = list(ctx.get("rules") or [])
        rules = [
            "Do not invent facts, entities, citations, or evidence.",
            "If the required information is missing, abstain instead of guessing.",
        ] + rules
        ctx["rules"] = rules

        env = make_jinja_env(templates_dir)
        template = _template_for_task(ctx)
        ops = [
            RoleOperator(_default_role()),
            GoalOperator(_default_goal_template()),
            ConstraintsOperator(),
            AbstainOperator(_default_abstain_instruction()),
        ]
        prompt, meta = render_prompt(env, template, ctx, ops=ops)
        return BuiltPrompt(prompt=prompt, meta=meta)


class C3_AddSchema(Condition):
    id = "C3"

    def build(self, *, templates_dir: str, ctx: Dict[str, Any]) -> BuiltPrompt:
        ctx = _ensure_defaults(ctx)
        # schema_text must exist for JSON tasks; for reasoning tasks it may be empty
        env = make_jinja_env(templates_dir)
        template = _template_for_task(ctx)
        ops = [
            RoleOperator(_default_role()),
            GoalOperator(_default_goal_template()),
            ConstraintsOperator(),
            AbstainOperator(_default_abstain_instruction()),
        ]
        prompt, meta = render_prompt(env, template, ctx, ops=ops)
        return BuiltPrompt(prompt=prompt, meta=meta)


class C4_AddVerifier(Condition):
    id = "C4"

    def build(self, *, templates_dir: str, ctx: Dict[str, Any]) -> BuiltPrompt:
        ctx = _ensure_defaults(ctx)
        env = make_jinja_env(templates_dir)
        template = _template_for_task(ctx)
        ops = [
            RoleOperator(_default_role()),
            GoalOperator(_default_goal_template()),
            ConstraintsOperator(),
            AbstainOperator(_default_abstain_instruction()),
            VerifierOperator(),
        ]
        prompt, meta = render_prompt(env, template, ctx, ops=ops)
        return BuiltPrompt(prompt=prompt, meta=meta)


class C5_AddFewShot(Condition):
    id = "C5"

    def build(self, *, templates_dir: str, ctx: Dict[str, Any]) -> BuiltPrompt:
        ctx = _ensure_defaults(ctx)
        env = make_jinja_env(templates_dir)
        template = _template_for_task(ctx)
        ops = [
            RoleOperator(_default_role()),
            GoalOperator(_default_goal_template()),
            ConstraintsOperator(),
            AbstainOperator(_default_abstain_instruction()),
            ExemplarsOperator(k=2),
            VerifierOperator(),
        ]
        prompt, meta = render_prompt(env, template, ctx, ops=ops)
        return BuiltPrompt(prompt=prompt, meta=meta)


class C6_HardConstraint(Condition):
    id = "C6"

    def build(self, *, templates_dir: str, ctx: Dict[str, Any]) -> BuiltPrompt:
        """
        C6 uses the same prompt structure as C4/C5 but asks the runner to enforce
        a JSON schema / grammar at decode time via Outlines/Guidance/LMQL.
        """
        ctx = _ensure_defaults(ctx)
        env = make_jinja_env(templates_dir)
        template = _template_for_task(ctx)

        # Prefer fewer-shot here to isolate constraint effect; keep exemplars optional
        ops = [
            RoleOperator(_default_role()),
            GoalOperator(_default_goal_template()),
            ConstraintsOperator(),
            AbstainOperator(_default_abstain_instruction()),
            VerifierOperator(),
        ]
        prompt, meta = render_prompt(env, template, ctx, ops=ops)

        constraint = None
        # If schema_text present, we assume JSON schema constraint; else no constraint
        if ctx.get("schema_text"):
            constraint = {"type": "json_schema", "schema_text": ctx["schema_text"]}

        return BuiltPrompt(prompt=prompt, meta=meta, constraint=constraint)


class C7_ClosedLoop_ReAct(Condition):
    id = "C7"

    def build(self, *, templates_dir: str, ctx: Dict[str, Any]) -> BuiltPrompt:
        """
        Runner should interpret `react` as an instruction to run an act-observe loop.
        We keep prompt short and policy-like.
        """
        ctx = _ensure_defaults(ctx)
        # ReAct is mainly meaningful for retrieval/verification tasks; but can be used elsewhere.
        env = make_jinja_env(templates_dir)
        template = _template_for_task(ctx)

        # Keep reasoning controlled; for reasoning tasks use brief to reduce verbosity
        reasoning_mode = "brief" if ctx.get("task_kind") in ("reasoning", "mcq") else "none"
        ops = [
            RoleOperator("You are a careful investigator. Use tools/observations to verify before answering."),
            GoalOperator(_default_goal_template()),
            ConstraintsOperator(),
            AbstainOperator(_default_abstain_instruction()),
            ReasoningModeOperator(reasoning_mode),
        ]
        prompt, meta = render_prompt(env, template, ctx, ops=ops)

        react = {
            "enabled": True,
            "max_steps": 5,
            "stop_when_confident": True,
            "tool_policy": "retrieve_then_answer",  # runner interprets this
        }
        return BuiltPrompt(prompt=prompt, meta=meta, react=react)


class C8_DSPyOptimized(Condition):
    id = "C8"

    def build(self, *, templates_dir: str, ctx: Dict[str, Any]) -> BuiltPrompt:
        """
        Placeholder: the DSPy optimizer should produce a 'compiled' prompt/program.
        The runner should replace ctx['compiled_prompt'] when optimization is enabled.
        """
        ctx = _ensure_defaults(ctx)
        compiled = ctx.get("compiled_prompt")
        if compiled:
            return BuiltPrompt(prompt=compiled.strip() + "\n", meta={"template": "compiled", "operators": ["DSPY_COMPILED"]})

        # Fallback to C4 structure if not compiled yet.
        return C4_AddVerifier().build(templates_dir=templates_dir, ctx=ctx)


# Registry
CONDITIONS = {
    "C0": C0_Baseline(),
    "C1": C1_RoleGoal(),
    "C2": C2_AddConstraints(),
    "C3": C3_AddSchema(),
    "C4": C4_AddVerifier(),
    "C5": C5_AddFewShot(),
    "C6": C6_HardConstraint(),
    "C7": C7_ClosedLoop_ReAct(),
    "C8": C8_DSPyOptimized(),
}


def build_condition(condition_id: str, *, templates_dir: str, ctx: Dict[str, Any]) -> BuiltPrompt:
    cond = CONDITIONS.get(condition_id)
    if not cond:
        raise KeyError(f"Unknown condition: {condition_id}")
    return cond.build(templates_dir=templates_dir, ctx=ctx)
5) Quick usage snippet (so you can test immediately)

If you want a sanity check without the full runner:

from pcsuite.prompts.conditions.conditions import build_condition

templates_dir = "src/pcsuite/prompts/base_templates"

ctx = {
  "task_id": "conll03_ner_json",
  "task_kind": "structured_extraction",
  "task_title": "NER Extraction",
  "task_goal": "Extract named entities from the input sentence.",
  "input_text": "EU rejects German call to boycott British lamb .",
  "rules": ["Use entity types: PER, ORG, LOC, MISC."],
  "schema_text": """{
    "type": "object",
    "properties": {
      "entities": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "text": {"type": "string"},
            "type": {"type": "string"},
            "start": {"type": "integer"},
            "end": {"type": "integer"}
          },
          "required": ["text","type","start","end"],
          "additionalProperties": false
        }
      }
    },
    "required": ["entities"],
    "additionalProperties": false
  }"""
}

bp = build_condition("C4", templates_dir=templates_dir, ctx=ctx)
print(bp.prompt)
print(bp.meta)
print("constraint:", bp.constraint)
print("react:", bp.react)






0) Minimal dependencies (pyproject)

Path: pyproject.toml

[project]
name = "prompt-control-suite"
version = "0.1.0"
requires-python = ">=3.10"
dependencies = [
  "pyyaml>=6.0",
  "jinja2>=3.1",
  "tqdm>=4.66",
  "numpy>=1.26",
  "pandas>=2.2",
  "matplotlib>=3.8",
]

[project.optional-dependencies]
hf = ["torch>=2.2", "transformers>=4.40"]
openai = ["openai>=1.40"]
outlines = ["outlines>=0.0.45", "jsonschema>=4.22"]
guidance = ["guidance>=0.1.16"]
lmql = ["lmql>=0.7.3"]
1) CLI entrypoint

Path: src/pcsuite/cli.py

from __future__ import annotations

import argparse
import sys

from pcsuite.runners.run_suite import run_suite
from pcsuite.runners.run_task import run_task
from pcsuite.logging.aggregate import aggregate_run
from pcsuite.logging.plots import make_plots


def main(argv: list[str] | None = None) -> int:
    p = argparse.ArgumentParser(prog="pcsuite")
    sub = p.add_subparsers(dest="cmd", required=True)

    p_run = sub.add_parser("run", help="Run full suite")
    p_run.add_argument("--config", required=True)

    p_task = sub.add_parser("run-task", help="Run a single task (debug)")
    p_task.add_argument("--config", required=True)
    p_task.add_argument("--task", required=True)
    p_task.add_argument("--model", required=True)

    p_report = sub.add_parser("report", help="Aggregate + plots")
    p_report.add_argument("--config", required=True)

    args = p.parse_args(argv)

    if args.cmd == "run":
        run_suite(args.config)
        return 0

    if args.cmd == "run-task":
        run_task(args.config, task_id=args.task, model_id=args.model)
        return 0

    if args.cmd == "report":
        run_name, out_dir = aggregate_run(args.config)
        make_plots(out_dir, run_name)
        return 0

    return 2


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
2) Config loader + registry
2.1 config.py

Path: src/pcsuite/config.py

from __future__ import annotations

import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml


def _read_yaml(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


@dataclass(frozen=True)
class Profile:
    raw: Dict[str, Any]

    @property
    def name(self) -> str:
        return self.raw["run"]["name"]

    @property
    def output_dir(self) -> str:
        return self.raw["run"]["output_dir"]

    @property
    def cache_dir(self) -> str:
        return self.raw["run"].get("cache_dir", ".cache")

    @property
    def overwrite(self) -> bool:
        return bool(self.raw["run"].get("overwrite", False))

    @property
    def global_seed(self) -> int:
        return int(self.raw["repro"]["global_seed"])

    def get(self, *keys: str, default: Any = None) -> Any:
        d: Any = self.raw
        for k in keys:
            if not isinstance(d, dict) or k not in d:
                return default
            d = d[k]
        return d

    def tasks_ref(self) -> str:
        return self.raw["suite"]["tasks_ref"]

    def models_ref(self) -> str:
        return self.raw["suite"]["models_ref"]

    def decoding_ref(self) -> str:
        return self.raw["suite"]["decoding_ref"]

    def operators_ref(self) -> str:
        return self.raw["suite"]["operators_ref"]

    def metrics_ref(self) -> str:
        return self.raw["suite"]["metrics_ref"]

    def selected_tasks(self) -> List[str]:
        return list(self.raw["selection"]["tasks"])

    def selected_models(self) -> List[str]:
        return list(self.raw["selection"]["models"])

    def selected_conditions(self) -> List[str]:
        return list(self.raw["selection"]["conditions"])

    def selected_decoding(self) -> List[str]:
        return list(self.raw["selection"]["decoding_regimes"])

    def max_items_per_task(self) -> Optional[int]:
        v = self.raw["limits"].get("max_items_per_task", None)
        return None if v is None else int(v)

    def stop_on_error_rate(self) -> float:
        return float(self.raw["limits"].get("stop_on_error_rate", 0.02))

    def max_inflight(self) -> int:
        return int(self.raw["repro"].get("max_inflight", 8))

    def num_workers(self) -> int:
        return int(self.raw["repro"].get("num_workers", 4))

    def seeds_for(self, regime: str) -> List[int]:
        if regime == "D0":
            return list(self.raw["repro"]["seeds"]["d0_greedy"])
        return list(self.raw["repro"]["seeds"]["d1_sampled"])


@dataclass(frozen=True)
class SuiteDefs:
    tasks: Dict[str, Any]
    models: Dict[str, Any]
    decoding: Dict[str, Any]


def load_profile(profile_path: str) -> Profile:
    prof = Profile(_read_yaml(profile_path))

    # Resolve refs relative to profile file
    base = Path(profile_path).parent
    suite = prof.raw["suite"]
    for key in ("tasks_ref", "models_ref", "operators_ref", "decoding_ref", "metrics_ref"):
        suite[key] = str((base / suite[key]).resolve())

    # Normalize output dir
    out = Path(prof.output_dir)
    if not out.is_absolute():
        prof.raw["run"]["output_dir"] = str((base / out).resolve())

    cache = Path(prof.cache_dir)
    if not cache.is_absolute():
        prof.raw["run"]["cache_dir"] = str((base / cache).resolve())

    return prof


def load_suite_defs(prof: Profile) -> SuiteDefs:
    tasks_raw = _read_yaml(prof.tasks_ref())
    models_raw = _read_yaml(prof.models_ref())
    dec_raw = _read_yaml(prof.decoding_ref())
    return SuiteDefs(
        tasks=tasks_raw["tasks"],
        models=models_raw["models"],
        decoding=dec_raw["decoding"],
    )
2.2 registry.py

Path: src/pcsuite/registry.py

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict

from pcsuite.engines.adapters.base import ModelAdapter
from pcsuite.engines.adapters.local_hf import LocalHFAdapter
from pcsuite.engines.adapters.openai_api import OpenAIAdapter


@dataclass
class ModelRegistry:
    adapters: Dict[str, ModelAdapter]

    @classmethod
    def from_models_yaml(cls, models: Dict[str, Any]) -> "ModelRegistry":
        adapters: Dict[str, ModelAdapter] = {}
        for model_id, spec in models.items():
            provider = spec.get("provider")
            if provider == "hf":
                adapters[model_id] = LocalHFAdapter(model_id=model_id, spec=spec)
            elif provider == "openai":
                adapters[model_id] = OpenAIAdapter(model_id=model_id, spec=spec)
            else:
                raise ValueError(f"Unknown model provider for {model_id}: {provider}")
        return cls(adapters=adapters)
3) Model adapters
3.1 Base adapter

Path: src/pcsuite/engines/adapters/base.py

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict


@dataclass
class ModelAdapter:
    id: str
    family: str
    size: str
    provider: str

    def generate(self, prompt: str, decoding: Dict[str, Any], seed: int) -> Dict[str, Any]:
        raise NotImplementedError
3.2 OpenAI adapter (optional)

Path: src/pcsuite/engines/adapters/openai_api.py

from __future__ import annotations

import time
from typing import Any, Dict

from pcsuite.engines.adapters.base import ModelAdapter


class OpenAIAdapter(ModelAdapter):
    def __init__(self, model_id: str, spec: Dict[str, Any]):
        super().__init__(
            id=model_id,
            family=str(spec.get("family", "api")),
            size=str(spec.get("size", "api")),
            provider="openai",
        )
        self.model_name = str(spec["model"])

    def generate(self, prompt: str, decoding: Dict[str, Any], seed: int) -> Dict[str, Any]:
        try:
            from openai import OpenAI  # type: ignore
        except Exception as e:
            raise RuntimeError("OpenAIAdapter requires `pip install .[openai]`") from e

        client = OpenAI()

        t0 = time.time()
        # Use Responses API shape; keep it minimal and stable
        resp = client.responses.create(
            model=self.model_name,
            input=prompt,
            temperature=float(decoding.get("temperature", 0.0)),
            top_p=float(decoding.get("top_p", 1.0)),
            max_output_tokens=int(decoding.get("max_tokens", 256)),
        )
        dt_ms = int((time.time() - t0) * 1000)

        text = ""
        if resp.output:
            # concatenate text segments
            for o in resp.output:
                if getattr(o, "content", None):
                    for c in o.content:
                        if getattr(c, "type", "") == "output_text":
                            text += getattr(c, "text", "")

        usage = getattr(resp, "usage", None)
        usage_dict = {
            "prompt_tokens": getattr(usage, "input_tokens", None),
            "completion_tokens": getattr(usage, "output_tokens", None),
            "total_tokens": getattr(usage, "total_tokens", None),
        }

        return {
            "text": text,
            "finish_reason": "stop",
            "latency_ms": dt_ms,
            "usage": usage_dict,
        }
3.3 Local HF adapter (optional)

Path: src/pcsuite/engines/adapters/local_hf.py

from __future__ import annotations

import time
from typing import Any, Dict

from pcsuite.engines.adapters.base import ModelAdapter


class LocalHFAdapter(ModelAdapter):
    def __init__(self, model_id: str, spec: Dict[str, Any]):
        super().__init__(
            id=model_id,
            family=str(spec.get("family", "hf")),
            size=str(spec.get("size", "local")),
            provider="hf",
        )
        self.model_path = str(spec["model_name_or_path"])
        self.device = str(spec.get("device", "cpu"))
        self.dtype = str(spec.get("dtype", "auto"))

        self._pipe = None  # lazy init

    def _ensure(self):
        if self._pipe is not None:
            return
        try:
            import torch  # type: ignore
            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # type: ignore
        except Exception as e:
            raise RuntimeError("LocalHFAdapter requires `pip install .[hf]`") from e

        tok = AutoTokenizer.from_pretrained(self.model_path, use_fast=True)
        model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=getattr(torch, self.dtype) if hasattr(torch, self.dtype) else None,
            device_map="auto" if self.device == "cuda" else None,
        )
        self._pipe = pipeline("text-generation", model=model, tokenizer=tok)

    def generate(self, prompt: str, decoding: Dict[str, Any], seed: int) -> Dict[str, Any]:
        self._ensure()
        import torch  # type: ignore

        torch.manual_seed(int(seed))

        t0 = time.time()
        max_new = int(decoding.get("max_tokens", 256))
        temperature = float(decoding.get("temperature", 0.0))
        top_p = float(decoding.get("top_p", 1.0))

        do_sample = temperature > 0.0

        out = self._pipe(  # type: ignore
            prompt,
            max_new_tokens=max_new,
            do_sample=do_sample,
            temperature=temperature if do_sample else None,
            top_p=top_p if do_sample else None,
            return_full_text=False,
        )
        dt_ms = int((time.time() - t0) * 1000)

        text = out[0]["generated_text"] if out else ""
        # HF pipeline doesnâ€™t expose token usage cleanly without extra hooks
        usage = {"prompt_tokens": None, "completion_tokens": None, "total_tokens": None}

        return {
            "text": text,
            "finish_reason": "stop",
            "latency_ms": dt_ms,
            "usage": usage,
        }
4) Constraint routing (C6 â†’ Outlines/Guidance)

Path: src/pcsuite/constraints/outlines_json.py

from __future__ import annotations

import time
from typing import Any, Dict

from pcsuite.engines.adapters.base import ModelAdapter


def generate_json_constrained(
    adapter: ModelAdapter,
    prompt: str,
    decoding: Dict[str, Any],
    schema_text: str,
    seed: int,
) -> Dict[str, Any]:
    """
    Best-effort Outlines JSON-schema constrained decoding.
    Falls back to adapter.generate if Outlines is not installed.
    """
    try:
        import json
        import outlines  # type: ignore
        from jsonschema import Draft202012Validator  # type: ignore
    except Exception:
        # fallback (still logs constraint backend = "fallback")
        out = adapter.generate(prompt, decoding, seed)
        out["constraint_backend"] = "fallback"
        return out

    # Parse schema
    schema = json.loads(schema_text)
    Draft202012Validator.check_schema(schema)

    # Outlines currently supports certain backends; for APIs, this may not work.
    # We only apply Outlines if adapter is a local HF adapter (provider == hf).
    if adapter.provider != "hf":
        out = adapter.generate(prompt, decoding, seed)
        out["constraint_backend"] = "fallback_nonlocal"
        return out

    # Need direct HF model/tokenizer; simplest: use outlines with Transformers
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM  # type: ignore
        import torch  # type: ignore
    except Exception:
        out = adapter.generate(prompt, decoding, seed)
        out["constraint_backend"] = "fallback_nohf"
        return out

    model_path = getattr(adapter, "model_path", None)
    if not model_path:
        out = adapter.generate(prompt, decoding, seed)
        out["constraint_backend"] = "fallback_nomodelpath"
        return out

    tok = AutoTokenizer.from_pretrained(model_path, use_fast=True)
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto")

    generator = outlines.generate.json(model, tok, schema)

    t0 = time.time()
    # Outlines handles decoding internally; pass temperature/top_p where supported
    torch.manual_seed(int(seed))
    text_obj = generator(
        prompt,
        max_tokens=int(decoding.get("max_tokens", 256)),
        temperature=float(decoding.get("temperature", 0.0)),
        top_p=float(decoding.get("top_p", 1.0)),
    )
    dt_ms = int((time.time() - t0) * 1000)

    return {
        "text": (text_obj if isinstance(text_obj, str) else str(text_obj)),
        "finish_reason": "stop",
        "latency_ms": dt_ms,
        "usage": {"prompt_tokens": None, "completion_tokens": None, "total_tokens": None},
        "constraint_backend": "outlines_json",
    }

Runner will call this when C6 constraint type is json_schema.

5) ReAct loop (C7) with retrieval
5.1 Simple local retrieval (token overlap BM25-ish)

Path: src/pcsuite/evaluation/fever.py

from __future__ import annotations

import re
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple


def _tok(s: str) -> List[str]:
    return re.findall(r"[a-z0-9]+", s.lower())


@dataclass
class Passage:
    pid: str
    text: str


class SimpleRetriever:
    """
    Minimal, deterministic retriever using token overlap scoring.
    Store passages in memory. Good enough to make C7 real without heavy deps.
    """
    def __init__(self, passages: List[Passage]):
        self.passages = passages
        self._ptoks = [set(_tok(p.text)) for p in passages]

    def search(self, query: str, k: int = 5) -> List[Passage]:
        qt = set(_tok(query))
        scored: List[Tuple[float, int]] = []
        for i, pt in enumerate(self._ptoks):
            if not pt:
                continue
            # overlap / sqrt len as a mild normalization
            inter = len(qt & pt)
            score = inter / (len(pt) ** 0.5)
            scored.append((score, i))
        scored.sort(reverse=True, key=lambda x: x[0])
        out = []
        for score, idx in scored[:k]:
            if score <= 0:
                continue
            out.append(self.passages[idx])
        return out


def format_observations(passages: List[Passage]) -> str:
    if not passages:
        return "OBSERVATION: (no relevant evidence found)\n"
    lines = ["OBSERVATION: Evidence snippets:"]
    for p in passages:
        lines.append(f"- [{p.pid}] {p.text}")
    return "\n".join(lines) + "\n"
5.2 ReAct runner loop

Path: src/pcsuite/runners/react_loop.py

from __future__ import annotations

import json
from typing import Any, Dict, Optional, Tuple

from pcsuite.engines.adapters.base import ModelAdapter
from pcsuite.evaluation.fever import SimpleRetriever, format_observations


REACT_INSTRUCTIONS = """You may use the tool RETRIEVE(query) to look up evidence.
When you are ready, output FINAL JSON only, matching the requested schema.
If evidence is insufficient, abstain.
"""


def _extract_retrieve_query(text: str) -> Optional[str]:
    # Extremely simple: look for RETRIEVE(...) in the model text
    start = text.find("RETRIEVE(")
    if start < 0:
        return None
    end = text.find(")", start)
    if end < 0:
        return None
    inside = text[start + len("RETRIEVE(") : end].strip().strip('"').strip("'")
    return inside or None


def run_react(
    adapter: ModelAdapter,
    base_prompt: str,
    decoding: Dict[str, Any],
    seed: int,
    retriever: Optional[SimpleRetriever],
    max_steps: int = 5,
) -> Dict[str, Any]:
    """
    Runs a minimal closed-loop:
      prompt -> model proposes RETRIEVE(query) or FINAL
      retrieve -> append observation -> repeat
    """
    transcript = base_prompt.rstrip() + "\n\n" + REACT_INSTRUCTIONS
    tool_calls = []

    for step in range(max_steps):
        out = adapter.generate(transcript, decoding, seed + step)
        text = out.get("text", "")

        q = _extract_retrieve_query(text)
        if q and retriever:
            passages = retriever.search(q, k=5)
            obs = format_observations(passages)
            tool_calls.append({"tool": "RETRIEVE", "query": q, "passage_ids": [p.pid for p in passages]})
            transcript += "\n" + text.strip() + "\n" + obs + "\n"
            continue

        # Otherwise assume it tried to answer; return
        out["react_steps"] = step + 1
        out["react_tool_calls"] = tool_calls
        out["react_transcript_chars"] = len(transcript)
        return out

    # If max steps reached, just return last output
    out = adapter.generate(transcript, decoding, seed + max_steps)
    out["react_steps"] = max_steps
    out["react_tool_calls"] = tool_calls
    out["react_transcript_chars"] = len(transcript)
    out["react_max_steps_hit"] = True
    return out
6) Logging: run_id hashing + JSONL writer
6.1 Hashing

Path: src/pcsuite/utils/hashing.py

from __future__ import annotations

import hashlib
import json
from typing import Any, Dict


def stable_hash(obj: Dict[str, Any]) -> str:
    s = json.dumps(obj, sort_keys=True, ensure_ascii=False, separators=(",", ":"))
    return hashlib.sha256(s.encode("utf-8")).hexdigest()
6.2 JSONL writer

Path: src/pcsuite/logging/writer.py

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict


class JsonlWriter:
    def __init__(self, path: str, overwrite: bool):
        self.path = Path(path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
        self._seen = set()

        if overwrite and self.path.exists():
            self.path.unlink()

        # Load existing run_ids if resuming
        if self.path.exists():
            with self.path.open("r", encoding="utf-8") as f:
                for line in f:
                    try:
                        row = json.loads(line)
                        rid = row.get("run_id")
                        if rid:
                            self._seen.add(rid)
                    except Exception:
                        continue

        self._fh = self.path.open("a", encoding="utf-8")

    def seen(self, run_id: str) -> bool:
        return run_id in self._seen

    def write(self, row: Dict[str, Any]) -> None:
        rid = row.get("run_id")
        if rid:
            self._seen.add(rid)
        self._fh.write(json.dumps(row, ensure_ascii=False) + "\n")
        self._fh.flush()

    def close(self) -> None:
        self._fh.close()
7) Task runner (core glue)

Path: src/pcsuite/runners/run_task.py

from __future__ import annotations

import json
import random
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

from tqdm import tqdm

from pcsuite.config import load_profile, load_suite_defs
from pcsuite.registry import ModelRegistry
from pcsuite.logging.writer import JsonlWriter
from pcsuite.utils.hashing import stable_hash
from pcsuite.prompts.conditions.conditions import build_condition
from pcsuite.constraints.outlines_json import generate_json_constrained
from pcsuite.runners.react_loop import run_react
from pcsuite.evaluation.fever import Passage, SimpleRetriever


def _utc_now() -> str:
    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")


def _det_sample(items: List[Dict[str, Any]], k: Optional[int], seed: int) -> List[Dict[str, Any]]:
    if k is None or k >= len(items):
        return items
    rng = random.Random(seed)
    idxs = list(range(len(items)))
    rng.shuffle(idxs)
    idxs = idxs[:k]
    return [items[i] for i in idxs]


def _load_dataset_stub(task_spec: Dict[str, Any], cache_dir: str, max_items: Optional[int], seed: int) -> List[Dict[str, Any]]:
    """
    Minimal loader stub:
      - expects cache_dir to contain pre-exported JSONL files named <dataset>_<split>.jsonl
      - each line must include fields: item_id, input_text, truth (dict)
    This keeps the runner runnable without pulling HuggingFace datasets.
    You can replace this later with real dataset loaders, keeping the runner unchanged.
    """
    dataset = task_spec["dataset"]
    # choose split key
    split_map = task_spec["split"]
    split = split_map.get("test") or split_map.get("dev") or next(iter(split_map.values()))
    path = Path(cache_dir) / "datasets" / f"{dataset}_{split}.jsonl"
    if not path.exists():
        raise FileNotFoundError(
            f"Dataset file not found: {path}\n"
            f"Expected JSONL with fields: item_id, input_text, truth.\n"
            f"Put exports in: {Path(cache_dir)/'datasets'}"
        )
    items = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            row = json.loads(line)
            items.append(row)
    items = _det_sample(items, max_items, seed=seed)
    return items


def _parse_json_best_effort(text: str) -> Dict[str, Any]:
    text = text.strip()
    # Try direct parse
    try:
        return {"json": json.loads(text), "json_valid": True, "errors": []}
    except Exception:
        pass
    # Try to extract first {...} block
    start = text.find("{")
    end = text.rfind("}")
    if start >= 0 and end > start:
        chunk = text[start : end + 1]
        try:
            return {"json": json.loads(chunk), "json_valid": True, "errors": ["extracted_json_block"]}
        except Exception as e:
            return {"json": None, "json_valid": False, "errors": [f"json_parse_error:{e}"]}
    return {"json": None, "json_valid": False, "errors": ["no_json_object_found"]}


def _score_stub(task_id: str, task_kind: str, parsed: Dict[str, Any], truth: Dict[str, Any]) -> Dict[str, Any]:
    """
    Minimal scoring to keep the suite operational:
      - structured/classification/mcq/verification: if truth has 'label' compare
      - reasoning: if truth has 'answer' compare extracted final
      - json_valid always logged
    Youâ€™ll replace/extend with task-specific evaluators later.
    """
    metrics: Dict[str, Any] = {
        "json_valid": parsed.get("json_valid"),
        "task_score": None,
        "exact_match": None,
        "unsupported_rate": None,
        "entropy_cat": None,
        "token_cost": None,
    }

    j = parsed.get("json")
    if task_kind in ("classification", "mcq", "verification", "truthfulness", "structured_extraction"):
        if isinstance(truth, dict) and "label" in truth and isinstance(j, dict) and "label" in j:
            metrics["exact_match"] = 1.0 if str(j["label"]).strip() == str(truth["label"]).strip() else 0.0
            metrics["task_score"] = metrics["exact_match"]
        else:
            # fallback: cannot score
            metrics["task_score"] = None
    else:
        # reasoning-like: compare truth["answer"] with extracted numeric/string
        if isinstance(truth, dict) and "answer" in truth:
            ans = str(truth["answer"]).strip()
            got = None
            if isinstance(j, dict) and "final_answer" in j:
                got = str(j["final_answer"]).strip()
            else:
                got = None
            if got is not None:
                metrics["exact_match"] = 1.0 if got == ans else 0.0
                metrics["task_score"] = metrics["exact_match"]

    return metrics


def _build_retriever_if_needed(profile_raw: Dict[str, Any], cache_dir: str) -> Optional[SimpleRetriever]:
    integ = profile_raw.get("integrations", {}).get("retrieval", {})
    if not integ:
        return None
    # expects cache_dir/wiki_passages.jsonl with fields: pid, text
    path = Path(cache_dir) / "wiki_passages.jsonl"
    if not path.exists():
        return None
    passages: List[Passage] = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            row = json.loads(line)
            passages.append(Passage(pid=str(row["pid"]), text=str(row["text"])))
    if not passages:
        return None
    return SimpleRetriever(passages)


def run_task(profile_path: str, *, task_id: str, model_id: str) -> None:
    prof = load_profile(profile_path)
    defs = load_suite_defs(prof)
    tasks = defs.tasks
    models = defs.models
    decoding_defs = defs.decoding

    if task_id not in tasks:
        raise KeyError(f"Unknown task_id: {task_id}")
    if model_id not in models:
        raise KeyError(f"Unknown model_id: {model_id}")

    out_dir = Path(prof.output_dir)
    run_dir = out_dir / "runs"
    run_dir.mkdir(parents=True, exist_ok=True)

    writer = JsonlWriter(str(run_dir / f"{prof.name}.jsonl"), overwrite=prof.overwrite)

    registry = ModelRegistry.from_models_yaml({model_id: models[model_id]})
    adapter = registry.adapters[model_id]

    task_spec = tasks[task_id]
    items = _load_dataset_stub(task_spec, prof.cache_dir, prof.max_items_per_task(), seed=prof.global_seed)

    templates_dir = str((Path(__file__).parents[1] / "prompts" / "base_templates").resolve())
    retriever = _build_retriever_if_needed(prof.raw, prof.cache_dir)

    total = 0
    errors = 0

    for item in tqdm(items, desc=f"{task_id}::{model_id}"):
        input_text = item.get("input_text", "")
        truth = item.get("truth", {}) or {}
        item_id = item.get("item_id", "unknown")

        # Build a basic ctx understood by our templates + conditions
        ctx_base: Dict[str, Any] = {
            "task_id": task_id,
            "task_kind": task_spec.get("kind", "reasoning"),
            "task_title": task_id,
            "task_goal": f"Complete {task_id} correctly.",
            "input_text": input_text,
            "rules": [],
            "schema_text": item.get("schema_text", "") or item.get("schema_text", task_spec.get("schema_text", "")) or "",
            "exemplars": item.get("exemplars", []) or [],
            "verifier_checklist": item.get("verifier_checklist", []) or [],
            "final_answer_format": item.get("final_answer_format", None),
        }

        for cond_id in prof.selected_conditions():
            built = build_condition(cond_id, templates_dir=templates_dir, ctx=ctx_base)

            for dec_id in prof.selected_decoding():
                dec = dict(decoding_defs[dec_id])
                dec["max_tokens"] = int(task_spec.get("max_tokens", dec.get("max_tokens", 256)))

                seeds = prof.seeds_for(dec_id)
                for seed in seeds:
                    total += 1
                    run_fingerprint = {
                        "profile": prof.name,
                        "task": {"id": task_id, "item_id": item_id},
                        "model": {"id": model_id},
                        "condition": {"id": cond_id},
                        "decoding": {"id": dec_id, "temperature": dec.get("temperature"), "top_p": dec.get("top_p"), "max_tokens": dec.get("max_tokens")},
                        "seed": seed,
                        "perturbation": {"type": None, "id": None},
                        "prompt_hash": stable_hash({"p": built.prompt}),
                        "schema_hash": stable_hash({"s": built.constraint}) if built.constraint else None,
                    }
                    run_id = stable_hash(run_fingerprint)
                    if writer.seen(run_id):
                        continue

                    try:
                        # Route:
                        # - C7: ReAct loop if built.react present
                        # - C6: constrained decoding if built.constraint present
                        # - else normal generate
                        gen_out: Dict[str, Any]
                        if built.react and built.react.get("enabled"):
                            gen_out = run_react(
                                adapter=adapter,
                                base_prompt=built.prompt,
                                decoding=dec,
                                seed=seed,
                                retriever=retriever,
                                max_steps=int(built.react.get("max_steps", 5)),
                            )
                            constraint_backend = "react_loop"
                        elif built.constraint and built.constraint.get("type") == "json_schema":
                            gen_out = generate_json_constrained(
                                adapter=adapter,
                                prompt=built.prompt,
                                decoding=dec,
                                schema_text=str(built.constraint.get("schema_text", "")),
                                seed=seed,
                            )
                            constraint_backend = gen_out.get("constraint_backend", "json_schema")
                        else:
                            gen_out = adapter.generate(built.prompt, dec, seed)
                            constraint_backend = None

                        parsed = _parse_json_best_effort(gen_out.get("text", "")) if task_spec.get("kind") != "reasoning" else {"json": None, "json_valid": None, "errors": []}
                        metrics = _score_stub(task_id, task_spec.get("kind", "reasoning"), parsed, truth)

                        row = {
                            "run_id": run_id,
                            "timestamp_utc": _utc_now(),
                            "profile": prof.name,
                            "task": {"id": task_id, "split": "test", "item_id": item_id},
                            "model": {"id": model_id, "family": adapter.family, "size": adapter.size, "provider": adapter.provider},
                            "condition": {"id": cond_id, "operators": built.meta.get("operators", [])},
                            "decoding": {"id": dec_id, "temperature": dec.get("temperature"), "top_p": dec.get("top_p"), "max_tokens": dec.get("max_tokens")},
                            "seed": seed,
                            "perturbation": {"type": None, "id": None},
                            "prompt": {"prompt_hash": run_fingerprint["prompt_hash"], "template": built.meta.get("template", ""), "rendered_chars": len(built.prompt)},
                            "generation": {
                                "text": gen_out.get("text", ""),
                                "finish_reason": gen_out.get("finish_reason", "stop"),
                                "latency_ms": gen_out.get("latency_ms", None),
                                "usage": gen_out.get("usage", {}),
                                "constraint_backend": constraint_backend,
                                "react_steps": gen_out.get("react_steps", None),
                            },
                            "parsed": {"json_valid": parsed.get("json_valid"), "fields": parsed.get("json"), "parse_errors": parsed.get("errors", [])},
                            "metrics": metrics,
                            "errors": [],
                        }
                        writer.write(row)

                    except Exception as e:
                        errors += 1
                        row = {
                            "run_id": run_id,
                            "timestamp_utc": _utc_now(),
                            "profile": prof.name,
                            "task": {"id": task_id, "split": "test", "item_id": item_id},
                            "model": {"id": model_id, "family": adapter.family, "size": adapter.size, "provider": adapter.provider},
                            "condition": {"id": cond_id, "operators": built.meta.get("operators", [])},
                            "decoding": {"id": dec_id, "temperature": dec.get("temperature"), "top_p": dec.get("top_p"), "max_tokens": dec.get("max_tokens")},
                            "seed": seed,
                            "perturbation": {"type": None, "id": None},
                            "prompt": {"prompt_hash": run_fingerprint["prompt_hash"], "template": built.meta.get("template", ""), "rendered_chars": len(built.prompt)},
                            "generation": {"text": "", "finish_reason": "error", "latency_ms": None, "usage": {}},
                            "parsed": {"json_valid": None, "fields": None, "parse_errors": []},
                            "metrics": {"task_score": None},
                            "errors": [repr(e)],
                        }
                        writer.write(row)

                    if total >= 50:
                        # error-rate safety abort for debugging / runaway failures
                        err_rate = errors / max(1, total)
                        if err_rate > prof.stop_on_error_rate():
                            writer.close()
                            raise RuntimeError(f"Aborting: error rate {err_rate:.3f} exceeded threshold {prof.stop_on_error_rate():.3f}")

    writer.close()

This is intentionally self-contained and robust. It will run as soon as you provide dataset JSONLs in .cache/datasets/ (see next note).

8) Suite runner (calls run_task repeatedly)

Path: src/pcsuite/runners/run_suite.py

from __future__ import annotations

from pcsuite.config import load_profile, load_suite_defs
from pcsuite.runners.run_task import run_task


def run_suite(profile_path: str) -> None:
    prof = load_profile(profile_path)
    defs = load_suite_defs(prof)

    for task_id in prof.selected_tasks():
        for model_id in prof.selected_models():
            run_task(profile_path, task_id=task_id, model_id=model_id)
9) Aggregation + plots (so â€œreportâ€ works)
9.1 Aggregate

Path: src/pcsuite/logging/aggregate.py

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Tuple

import pandas as pd

from pcsuite.config import load_profile


def aggregate_run(profile_path: str) -> Tuple[str, str]:
    prof = load_profile(profile_path)
    out_dir = Path(prof.output_dir)
    run_path = out_dir / "runs" / f"{prof.name}.jsonl"
    if not run_path.exists():
        raise FileNotFoundError(f"Missing run log: {run_path}")

    rows = []
    with run_path.open("r", encoding="utf-8") as f:
        for line in f:
            rows.append(json.loads(line))

    df = pd.json_normalize(rows)

    tables_dir = out_dir / "tables"
    tables_dir.mkdir(parents=True, exist_ok=True)

    df.to_parquet(tables_dir / "runs.parquet", index=False)
    df.to_csv(tables_dir / "runs.csv", index=False)

    # Summary by task/model/condition/decoding
    metric_cols = [c for c in df.columns if c.startswith("metrics.")]
    group_cols = ["task.id", "model.id", "condition.id", "decoding.id"]

    summary = df.groupby(group_cols, dropna=False)[metric_cols].mean(numeric_only=True).reset_index()
    summary.to_csv(tables_dir / "summary_by_condition.csv", index=False)

    return prof.name, str(out_dir)
9.2 Plots

Path: src/pcsuite/logging/plots.py

from __future__ import annotations

from pathlib import Path

import matplotlib.pyplot as plt
import pandas as pd


def make_plots(output_dir: str, run_name: str) -> None:
    out = Path(output_dir)
    tables = out / "tables"
    plots = out / "plots"
    plots.mkdir(parents=True, exist_ok=True)

    summary_path = tables / "summary_by_condition.csv"
    if not summary_path.exists():
        raise FileNotFoundError(summary_path)

    df = pd.read_csv(summary_path)

    # Plot: condition ladder of metrics.task_score if present
    if "metrics.task_score" in df.columns:
        for (task_id, model_id, dec_id), g in df.groupby(["task.id", "model.id", "decoding.id"]):
            g = g.sort_values("condition.id")
            plt.figure()
            plt.plot(g["condition.id"], g["metrics.task_score"], marker="o")
            plt.xlabel("Condition")
            plt.ylabel("Mean task_score")
            plt.title(f"{task_id} | {model_id} | {dec_id}")
            plt.tight_layout()
            plt.savefig(plots / f"perf_condition_ladder__{task_id}__{model_id}__{dec_id}.png")
            plt.close()

(Kept simple; you can expand into the full plotting plan once real metrics are wired.)

10) Dataset format (so it runs immediately)

Put JSONL exports here:

.cache/datasets/<dataset>_<split>.jsonl

Each line MUST be:

{
  "item_id": "unique-id",
  "input_text": "the task input",
  "truth": {"label": "..."}  // or {"answer": "..."} etc
}

Example file names the stub loader expects:

.cache/datasets/conll2003_test.jsonl

.cache/datasets/ag_news_test.jsonl

.cache/datasets/gsm8k_test.jsonl

.cache/datasets/commonsense_qa_dev.jsonl

.cache/datasets/truthful_qa_validation.jsonl

.cache/datasets/fever_dev.jsonl

If you want, I can also drop a dataset exporter script using datasets (HF) that generates these JSONLs automatically â€” but the runner above is deliberately not blocked on that.

11) Run it
python -m pcsuite.cli run --config configs/profile_A.yaml
python -m pcsuite.cli report --config configs/profile_A.yaml







1) Add optional dependency for dataset export

Update pyproject.toml to include datasets as an optional extra:

[project.optional-dependencies]
hf = ["torch>=2.2", "transformers>=4.40", "datasets>=2.20"]
openai = ["openai>=1.40"]
outlines = ["outlines>=0.0.45", "jsonschema>=4.22"]
guidance = ["guidance>=0.1.16"]
lmql = ["lmql>=0.7.3"]

Install:

pip install -e ".[hf]"
2) Dataset exporter script (creates .cache/datasets/*.jsonl)

Path: scripts/export_hf_datasets.py

#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

def _write_jsonl(path: Path, rows: Iterable[Dict[str, Any]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def _ensure_datasets():
    try:
        import datasets  # noqa
    except Exception as e:
        raise RuntimeError("This script requires: pip install -e '.[hf]' (needs datasets)") from e

def export_conll2003(out_dir: Path) -> None:
    _ensure_datasets()
    from datasets import load_dataset  # type: ignore

    ds = load_dataset("conll2003")
    # splits: train, validation, test
    def rows(split: str):
        for i, ex in enumerate(ds[split]):
            tokens = ex["tokens"]
            ner_tags = ex["ner_tags"]
            # We'll keep tokens + gold tags; scorer will compute spans
            yield {
                "item_id": f"conll2003:{split}:{i}",
                "input_text": " ".join(tokens),
                "truth": {"tokens": tokens, "ner_tags": ner_tags},
            }

    _write_jsonl(out_dir / "conll2003_train.jsonl", rows("train"))
    _write_jsonl(out_dir / "conll2003_validation.jsonl", rows("validation"))
    _write_jsonl(out_dir / "conll2003_test.jsonl", rows("test"))

def export_ag_news(out_dir: Path) -> None:
    _ensure_datasets()
    from datasets import load_dataset  # type: ignore

    ds = load_dataset("ag_news")
    labels = ["World", "Sports", "Business", "Sci/Tech"]

    def rows(split: str):
        for i, ex in enumerate(ds[split]):
            text = ex["text"]
            y = int(ex["label"])
            yield {
                "item_id": f"ag_news:{split}:{i}",
                "input_text": text,
                "truth": {"label": labels[y], "label_id": y},
            }

    _write_jsonl(out_dir / "ag_news_train.jsonl", rows("train"))
    _write_jsonl(out_dir / "ag_news_test.jsonl", rows("test"))

def export_gsm8k(out_dir: Path) -> None:
    _ensure_datasets()
    from datasets import load_dataset  # type: ignore

    ds = load_dataset("gsm8k", "main")
    # splits: train, test
    def rows(split: str):
        for i, ex in enumerate(ds[split]):
            q = ex["question"]
            a = ex["answer"]  # includes rationale + "#### <final>"
            yield {
                "item_id": f"gsm8k:{split}:{i}",
                "input_text": q,
                "truth": {"answer": a},
            }

    _write_jsonl(out_dir / "gsm8k_train.jsonl", rows("train"))
    _write_jsonl(out_dir / "gsm8k_test.jsonl", rows("test"))

def export_commonsenseqa(out_dir: Path) -> None:
    _ensure_datasets()
    from datasets import load_dataset  # type: ignore

    ds = load_dataset("commonsense_qa")
    # splits: train, validation, test (test has no label in some releases)
    def rows(split: str):
        for i, ex in enumerate(ds[split]):
            q = ex["question"]
            choices = ex["choices"]["text"]
            labels = ex["choices"]["label"]  # ["A","B","C","D","E"]
            # gold answerKey might be missing on test
            truth = {}
            if "answerKey" in ex and ex["answerKey"] is not None:
                truth["label"] = ex["answerKey"]
            truth["choices"] = list(zip(labels, choices))
            # Provide formatted prompt input text
            opts = "\n".join([f"{lab}. {txt}" for lab, txt in truth["choices"]])
            inp = f"{q}\n\nChoices:\n{opts}"
            yield {"item_id": f"commonsense_qa:{split}:{i}", "input_text": inp, "truth": truth}

    _write_jsonl(out_dir / "commonsense_qa_train.jsonl", rows("train"))
    _write_jsonl(out_dir / "commonsense_qa_validation.jsonl", rows("validation"))
    _write_jsonl(out_dir / "commonsense_qa_test.jsonl", rows("test"))

def export_truthfulqa(out_dir: Path) -> None:
    _ensure_datasets()
    from datasets import load_dataset  # type: ignore

    # HF commonly uses "truthful_qa" with configs: "generation" and/or "multiple_choice"
    # We'll export generation split(s) if present.
    # Some versions name split "validation" only.
    try:
        ds = load_dataset("truthful_qa", "generation")
        config = "generation"
    except Exception:
        ds = load_dataset("truthful_qa")
        config = "default"

    # Try typical splits
    split_candidates = [s for s in ["validation", "test", "train"] if s in ds]
    if not split_candidates:
        split_candidates = list(ds.keys())

    def rows(split: str):
        for i, ex in enumerate(ds[split]):
            q = ex.get("question") or ex.get("prompt") or ""
            # The dataset often includes 'best_answer', 'correct_answers', 'incorrect_answers'
            truth = {}
            if "best_answer" in ex:
                truth["best_answer"] = ex["best_answer"]
            if "correct_answers" in ex:
                truth["correct_answers"] = ex["correct_answers"]
            if "incorrect_answers" in ex:
                truth["incorrect_answers"] = ex["incorrect_answers"]
            yield {"item_id": f"truthful_qa_{config}:{split}:{i}", "input_text": q, "truth": truth}

    for split in split_candidates:
        _write_jsonl(out_dir / f"truthful_qa_{config}_{split}.jsonl", rows(split))

def export_fever(out_dir: Path) -> None:
    _ensure_datasets()
    from datasets import load_dataset  # type: ignore

    ds = load_dataset("fever", "v1.0")
    # splits: train, labelled_dev, unlabelled_test typically
    # We'll export labelled_dev by default.
    def rows(split: str):
        for i, ex in enumerate(ds[split]):
            claim = ex["claim"]
            label = ex.get("label")
            truth = {}
            if label is not None:
                truth["label"] = label
            # evidence is nested; keep raw for optional scoring
            if "evidence" in ex and ex["evidence"] is not None:
                truth["evidence"] = ex["evidence"]
            yield {"item_id": f"fever:{split}:{i}", "input_text": claim, "truth": truth}

    _write_jsonl(out_dir / "fever_train.jsonl", rows("train"))
    if "labelled_dev" in ds:
        _write_jsonl(out_dir / "fever_dev.jsonl", rows("labelled_dev"))
    else:
        # fallback
        _write_jsonl(out_dir / "fever_dev.jsonl", rows("train"))

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--cache-dir", default=".cache", help="Suite cache dir (same as profile run.cache_dir)")
    ap.add_argument("--only", default="", help="Comma-separated: conll,ag,gsm8k,csqa,truthfulqa,fever")
    args = ap.parse_args()

    out_dir = Path(args.cache_dir) / "datasets"
    only = [x.strip().lower() for x in args.only.split(",") if x.strip()]

    def want(name: str) -> bool:
        return (not only) or (name in only)

    if want("conll") or want("conll2003"):
        export_conll2003(out_dir)
    if want("ag") or want("agnews") or want("ag_news"):
        export_ag_news(out_dir)
    if want("gsm8k"):
        export_gsm8k(out_dir)
    if want("csqa") or want("commonsenseqa") or want("commonsense_qa"):
        export_commonsenseqa(out_dir)
    if want("truthfulqa") or want("truthful_qa"):
        export_truthfulqa(out_dir)
    if want("fever"):
        export_fever(out_dir)

    print(f"Export complete â†’ {out_dir}")

if __name__ == "__main__":
    main()

Run it:

python scripts/export_hf_datasets.py --cache-dir .cache
3) Real evaluators (replace stub scoring)
3.1 Evaluator registry + helpers

Path: src/pcsuite/evaluation/evaluators.py

from __future__ import annotations

import math
import re
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple


def _norm(s: str) -> str:
    return re.sub(r"\s+", " ", s.strip().lower())

def _extract_json_fields(parsed: Dict[str, Any]) -> Any:
    return parsed.get("json")

def _safe_str(x: Any) -> str:
    return "" if x is None else str(x)

# -----------------------------
# GSM8K
# -----------------------------
_GSM8K_FINAL_RE = re.compile(r"####\s*([-+]?\d[\d,]*\.?\d*)")
_NUM_RE = re.compile(r"[-+]?\d[\d,]*\.?\d*")

def gsm8k_gold_final(answer_field: str) -> Optional[str]:
    m = _GSM8K_FINAL_RE.search(answer_field or "")
    if m:
        return m.group(1).replace(",", "").strip()
    # fallback: last number in string
    nums = _NUM_RE.findall(answer_field or "")
    if nums:
        return nums[-1].replace(",", "").strip()
    return None

def gsm8k_pred_final(text: str) -> Optional[str]:
    # take last number in the model output
    nums = _NUM_RE.findall(text or "")
    if not nums:
        return None
    return nums[-1].replace(",", "").strip()

# -----------------------------
# CoNLL NER spans
# -----------------------------
# CoNLL2003 tag set in HF uses ints; but we only get ints in truth.
# We'll compute spans from gold tags + from predicted JSON entities.
# Predicted JSON expected shape: {"entities":[{"text","type","start","end"}...]}
# We will score by matching (start,end,type) using *token indices*.
#
# Since exporter gives input_text as " ".join(tokens), we also include tokens in truth.
#
# Gold tag mapping in HF conll2003: 0 O, then B/I tags for PER/ORG/LOC/MISC
# We'll derive spans directly from tag IDs using the dataset's feature names if present.
#
# To avoid requiring dataset features, we hardcode the canonical order used by conll2003:
CONLL_TAGS = [
    "O",
    "B-PER","I-PER",
    "B-ORG","I-ORG",
    "B-LOC","I-LOC",
    "B-MISC","I-MISC",
]

def conll_tags_to_spans(tags: List[int]) -> List[Tuple[int,int,str]]:
    spans: List[Tuple[int,int,str]] = []
    i = 0
    while i < len(tags):
        t = CONLL_TAGS[tags[i]] if 0 <= tags[i] < len(CONLL_TAGS) else "O"
        if t == "O":
            i += 1
            continue
        if t.startswith("B-"):
            ent_type = t[2:]
            start = i
            i += 1
            while i < len(tags):
                t2 = CONLL_TAGS[tags[i]] if 0 <= tags[i] < len(CONLL_TAGS) else "O"
                if t2 == f"I-{ent_type}":
                    i += 1
                else:
                    break
            end = i  # exclusive
            spans.append((start, end, ent_type))
        else:
            # stray I- tag; treat as B-
            ent_type = t[2:] if t.startswith("I-") else t
            start = i
            i += 1
            while i < len(tags):
                t2 = CONLL_TAGS[tags[i]] if 0 <= tags[i] < len(CONLL_TAGS) else "O"
                if t2 == f"I-{ent_type}":
                    i += 1
                else:
                    break
            end = i
            spans.append((start, end, ent_type))
    return spans

def conll_pred_to_spans(pred_json: Any, tokens: List[str]) -> Tuple[List[Tuple[int,int,str]], float]:
    """
    Best-effort: map predicted entities with 'start'/'end' token indices.
    If start/end are character indices instead, we can't reliably map without more work,
    so we treat them as missing.
    Returns (spans, unsupported_rate) where unsupported_rate counts entities whose text not in input.
    """
    if not isinstance(pred_json, dict):
        return [], 1.0
    ents = pred_json.get("entities")
    if not isinstance(ents, list):
        return [], 1.0
    joined = " ".join(tokens)
    bad = 0
    spans: List[Tuple[int,int,str]] = []
    for e in ents:
        if not isinstance(e, dict):
            bad += 1
            continue
        txt = _safe_str(e.get("text"))
        typ = _safe_str(e.get("type")).upper()
        s = e.get("start")
        en = e.get("end")
        if txt and txt not in joined:
            bad += 1
        if isinstance(s, int) and isinstance(en, int) and 0 <= s < en <= len(tokens):
            spans.append((s, en, typ))
        else:
            bad += 1
    unsupported_rate = bad / max(1, len(ents))
    return spans, unsupported_rate

def f1_spans(pred: List[Tuple[int,int,str]], gold: List[Tuple[int,int,str]]) -> Tuple[float,float,float]:
    ps = set(pred)
    gs = set(gold)
    tp = len(ps & gs)
    fp = len(ps - gs)
    fn = len(gs - ps)
    prec = tp / (tp + fp) if (tp + fp) else 0.0
    rec = tp / (tp + fn) if (tp + fn) else 0.0
    f1 = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0
    return prec, rec, f1

# -----------------------------
# TruthfulQA best-effort scoring
# -----------------------------
def truthfulqa_em(pred: str, truth: Dict[str, Any]) -> Dict[str, Any]:
    """
    Best-effort without the official TruthfulQA judge:
    - If correct_answers exist: EM if matches any (normalized substring)
    - If incorrect_answers exist: flag if matches an incorrect answer (as a proxy for untruthfulness)
    """
    p = _norm(pred)
    correct = [_norm(x) for x in (truth.get("correct_answers") or []) if isinstance(x, str)]
    incorrect = [_norm(x) for x in (truth.get("incorrect_answers") or []) if isinstance(x, str)]
    best = _norm(truth.get("best_answer")) if isinstance(truth.get("best_answer"), str) else ""

    em_correct = 0.0
    if best and best in p:
        em_correct = 1.0
    elif correct:
        em_correct = 1.0 if any(c and c in p for c in correct) else 0.0

    hit_incorrect = 1.0 if any(w and w in p for w in incorrect) else 0.0

    # "truthfulness_proxy": correct hit and not incorrect hit
    truth_proxy = 1.0 if (em_correct == 1.0 and hit_incorrect == 0.0) else 0.0
    return {"truthfulqa_em_correct": em_correct, "truthfulqa_hit_incorrect": hit_incorrect, "truthfulness_proxy": truth_proxy}

# -----------------------------
# FEVER
# -----------------------------
def fever_label_score(pred_json: Any, truth: Dict[str, Any]) -> Dict[str, Any]:
    gold = _safe_str(truth.get("label")).upper()
    if not isinstance(pred_json, dict):
        return {"fever_label_em": None, "unsupported_rate": None}
    pred = _safe_str(pred_json.get("label")).upper()
    em = 1.0 if gold and pred == gold else 0.0 if gold else None

    # Evidence scoring only if truth evidence is present and prediction supplies evidence list
    # We'll compute a weak overlap score over page IDs if present.
    overlap = None
    if "evidence" in truth and isinstance(pred_json.get("evidence"), list):
        gold_e = truth.get("evidence")
        pred_e = pred_json.get("evidence")
        gold_pages = set()
        try:
            # FEVER evidence format: list of groups, each group list of evidence items; items include page title at index 2
            for group in gold_e:
                for ev in group:
                    if isinstance(ev, list) and len(ev) >= 3 and isinstance(ev[2], str):
                        gold_pages.add(_norm(ev[2]))
        except Exception:
            gold_pages = set()

        pred_pages = set()
        for ev in pred_e:
            if isinstance(ev, dict) and "page" in ev:
                pred_pages.add(_norm(ev["page"]))
            elif isinstance(ev, str):
                pred_pages.add(_norm(ev))

        if gold_pages:
            tp = len(pred_pages & gold_pages)
            fp = len(pred_pages - gold_pages)
            fn = len(gold_pages - pred_pages)
            prec = tp / (tp + fp) if (tp + fp) else 0.0
            rec = tp / (tp + fn) if (tp + fn) else 0.0
            overlap = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0

    return {"fever_label_em": em, "fever_evidence_f1": overlap}

# -----------------------------
# Categorical entropy across sampled runs computed at aggregation time
# -----------------------------

@dataclass
class EvaluationResult:
    metrics: Dict[str, Any]
    parsed_fields: Any
    parse_errors: List[str]


class Evaluator:
    task_id: str
    task_kind: str

    def parse(self, text: str) -> EvaluationResult:
        raise NotImplementedError

    def score(self, parsed_fields: Any, text: str, truth: Dict[str, Any]) -> Dict[str, Any]:
        raise NotImplementedError


# -------- JSON-based tasks --------
import json

def parse_json_best_effort(text: str) -> EvaluationResult:
    t = (text or "").strip()
    try:
        return EvaluationResult(metrics={"json_valid": True}, parsed_fields=json.loads(t), parse_errors=[])
    except Exception:
        start = t.find("{")
        end = t.rfind("}")
        if start >= 0 and end > start:
            chunk = t[start:end+1]
            try:
                return EvaluationResult(metrics={"json_valid": True}, parsed_fields=json.loads(chunk), parse_errors=["extracted_json_block"])
            except Exception as e:
                return EvaluationResult(metrics={"json_valid": False}, parsed_fields=None, parse_errors=[f"json_parse_error:{e}"])
        return EvaluationResult(metrics={"json_valid": False}, parsed_fields=None, parse_errors=["no_json_object_found"])


class ConllNerJsonEvaluator(Evaluator):
    task_id = "conll03_ner_json"
    task_kind = "structured_extraction"

    def parse(self, text: str) -> EvaluationResult:
        return parse_json_best_effort(text)

    def score(self, parsed_fields: Any, text: str, truth: Dict[str, Any]) -> Dict[str, Any]:
        tokens = truth.get("tokens") or []
        tags = truth.get("ner_tags") or []
        gold_spans = conll_tags_to_spans(list(tags))
        pred_spans, unsupported_rate = conll_pred_to_spans(parsed_fields, list(tokens))
        prec, rec, f1 = f1_spans(pred_spans, gold_spans)
        return {
            "task_score": f1,
            "conll_precision": prec,
            "conll_recall": rec,
            "conll_f1": f1,
            "unsupported_rate": unsupported_rate,
        }


class AgNewsJsonEvaluator(Evaluator):
    task_id = "agnews_json"
    task_kind = "classification"

    def parse(self, text: str) -> EvaluationResult:
        return parse_json_best_effort(text)

    def score(self, parsed_fields: Any, text: str, truth: Dict[str, Any]) -> Dict[str, Any]:
        gold = _safe_str(truth.get("label")).strip()
        pred = ""
        if isinstance(parsed_fields, dict):
            pred = _safe_str(parsed_fields.get("label")).strip()
        em = 1.0 if gold and pred == gold else 0.0
        return {"task_score": em, "exact_match": em}


class CsqaJsonEvaluator(Evaluator):
    task_id = "csqa_json"
    task_kind = "mcq"

    def parse(self, text: str) -> EvaluationResult:
        return parse_json_best_effort(text)

    def score(self, parsed_fields: Any, text: str, truth: Dict[str, Any]) -> Dict[str, Any]:
        gold = _safe_str(truth.get("label")).strip().upper()
        pred = ""
        if isinstance(parsed_fields, dict):
            pred = _safe_str(parsed_fields.get("choice")).strip().upper()
        em = None
        if gold:
            em = 1.0 if pred == gold else 0.0
        return {"task_score": em, "exact_match": em}


class TruthfulQaJsonEvaluator(Evaluator):
    task_id = "truthfulqa_gen"
    task_kind = "truthfulness"

    def parse(self, text: str) -> EvaluationResult:
        # Some conditions will output JSON, but some may output plain text.
        # We accept JSON if possible; otherwise store the raw string in parsed_fields.
        ej = parse_json_best_effort(text)
        if ej.metrics.get("json_valid") and isinstance(ej.parsed_fields, dict) and "answer" in ej.parsed_fields:
            return ej
        return EvaluationResult(metrics={"json_valid": ej.metrics.get("json_valid")}, parsed_fields={"answer": text.strip(), "abstained": False}, parse_errors=ej.parse_errors)

    def score(self, parsed_fields: Any, text: str, truth: Dict[str, Any]) -> Dict[str, Any]:
        ans = ""
        if isinstance(parsed_fields, dict):
            ans = _safe_str(parsed_fields.get("answer"))
        s = truthfulqa_em(ans, truth)
        # Use truthfulness_proxy as task_score best-effort
        return {"task_score": s["truthfulness_proxy"], **s}


class FeverJsonEvaluator(Evaluator):
    task_id = "fever_json"
    task_kind = "verification"

    def parse(self, text: str) -> EvaluationResult:
        return parse_json_best_effort(text)

    def score(self, parsed_fields: Any, text: str, truth: Dict[str, Any]) -> Dict[str, Any]:
        s = fever_label_score(parsed_fields, truth)
        task_score = s.get("fever_label_em")
        return {"task_score": task_score, **s}


# -------- Reasoning (non-JSON) --------
class Gsm8kEvaluator(Evaluator):
    task_id = "gsm8k"
    task_kind = "reasoning"

    def parse(self, text: str) -> EvaluationResult:
        # keep raw
        return EvaluationResult(metrics={"json_valid": None}, parsed_fields={"raw": text}, parse_errors=[])

    def score(self, parsed_fields: Any, text: str, truth: Dict[str, Any]) -> Dict[str, Any]:
        gold = gsm8k_gold_final(_safe_str(truth.get("answer")))
        pred = gsm8k_pred_final(text)
        em = None
        if gold is not None and pred is not None:
            em = 1.0 if pred == gold else 0.0
        return {"task_score": em, "exact_match": em, "gsm8k_gold": gold, "gsm8k_pred": pred}


EVALUATORS: Dict[str, Evaluator] = {
    "conll03_ner_json": ConllNerJsonEvaluator(),
    "agnews_json": AgNewsJsonEvaluator(),
    "gsm8k": Gsm8kEvaluator(),
    "csqa_json": CsqaJsonEvaluator(),
    "truthfulqa_gen": TruthfulQaJsonEvaluator(),
    "fever_json": FeverJsonEvaluator(),
}

def get_evaluator(task_id: str) -> Evaluator:
    if task_id not in EVALUATORS:
        raise KeyError(f"No evaluator registered for task_id={task_id}")
    return EVALUATORS[task_id]
4) Wire real evaluators into the runner

Replace the stub parse/score parts in src/pcsuite/runners/run_task.py.

4.1 Minimal patch: import and use get_evaluator

Open src/pcsuite/runners/run_task.py and:

Add import near the top:

from pcsuite.evaluation.evaluators import get_evaluator

Delete these functions (they are now obsolete):

_parse_json_best_effort

_score_stub

In the main loop, after gen_out is computed, replace:

parsed = _parse_json_best_effort(...) ...
metrics = _score_stub(...)

with:

evaluator = get_evaluator(task_id)
ev = evaluator.parse(gen_out.get("text", ""))
metrics = {**ev.metrics, **evaluator.score(ev.parsed_fields, gen_out.get("text", ""), truth)}

Replace the row["parsed"] section with:

"parsed": {"json_valid": ev.metrics.get("json_valid"), "fields": ev.parsed_fields, "parse_errors": ev.parse_errors},
"metrics": metrics,

Thatâ€™s it â€” now youâ€™re doing real task scoring.

5) Make your tasks_v1.yaml match exporter filenames

Your stub loader in run_task.py looks for <dataset>_<split>.jsonl, where dataset comes from YAML.

So ensure these entries in configs/tasks_v1.yaml:

tasks:
  conll03_ner_json:
    dataset: "conll2003"
    split: {train: "train", dev: "validation", test: "test"}

  agnews_json:
    dataset: "ag_news"
    split: {train: "train", dev: "train", test: "test"}  # dev handling: you can sample 5k from train later

  gsm8k:
    dataset: "gsm8k"
    split: {dev: "train", test: "test"}

  csqa_json:
    dataset: "commonsense_qa"
    split: {dev: "validation"}

  truthfulqa_gen:
    dataset: "truthful_qa_generation"
    split: {test: "validation"}  # exporter writes truthful_qa_generation_validation.jsonl

  fever_json:
    dataset: "fever"
    split: {dev: "dev"}

Note: exporter names TruthfulQA files truthful_qa_generation_validation.jsonl.
So set dataset to truthful_qa_generation and split to validation.

If you want a cleaner mapping, I can adjust _load_dataset_stub to accept explicit filenames in YAML; but the above keeps it consistent with your existing stub.

6) Run end-to-end
python scripts/export_hf_datasets.py --cache-dir .cache
python -m pcsuite.cli run --config configs/profile_A.yaml
python -m pcsuite.cli report --config configs/profile_A.yaml





'''


[[What you now have (and why itâ€™s â€œthesis-gradeâ€)

Deterministic dataset exports â†’ reproducible

Real per-task scorers:

CoNLL03: micro span F1 + unsupported entity proxy

AG News: accuracy

GSM8K: exact match on canonical â€œ#### answerâ€

CSQA: accuracy (when labels available)

TruthfulQA: best-effort proxy (correct answer hit vs incorrect hit)

FEVER: label accuracy + optional weak evidence overlap

This is enough to run the full ladder C0â€“C8 and start generating meaningful plots and deltas.]]






...





1) Add evaluation dependencies

Update pyproject.toml with a new optional extra:

[project.optional-dependencies]
hf = ["torch>=2.2", "transformers>=4.40", "datasets>=2.20"]
openai = ["openai>=1.40"]
outlines = ["outlines>=0.0.45", "jsonschema>=4.22"]
guidance = ["guidance>=0.1.16"]
lmql = ["lmql>=0.7.3"]

# NEW: evaluation metrics like ROUGE/BLEU/BLEURT via evaluate
eval = ["evaluate>=0.4.2", "sacrebleu>=2.4.2", "rouge-score>=0.1.2"]

Install:

pip install -e ".[hf,eval]"
2) TruthfulQA scorer (reference-based, matches the repoâ€™s â€œsimilarity true vs falseâ€ logic)

This mirrors the TruthfulQA repositoryâ€™s approach: compute similarity to true references and subtract similarity to false references, then â€œaccâ€ is max(true) > max(false) for each metric.

2.1 New file: src/pcsuite/evaluation/truthfulqa_metrics.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple


def _norm_list(xs: Any) -> List[str]:
    if xs is None:
        return []
    if isinstance(xs, list):
        return [str(x) for x in xs if isinstance(x, (str, int, float))]
    if isinstance(xs, str):
        return [xs]
    return []


@dataclass
class TruthfulQAMetricResult:
    bleu_max_true: Optional[float]
    bleu_max_false: Optional[float]
    bleu_diff: Optional[float]
    bleu_acc: Optional[float]

    rougeL_max_true: Optional[float]
    rougeL_max_false: Optional[float]
    rougeL_diff: Optional[float]
    rougeL_acc: Optional[float]

    bleurt_max_true: Optional[float]
    bleurt_max_false: Optional[float]
    bleurt_diff: Optional[float]
    bleurt_acc: Optional[float]


def _try_load_eval(metric_name: str):
    try:
        import evaluate  # type: ignore
    except Exception as e:
        raise RuntimeError("Install eval extras: pip install -e '.[eval]'") from e

    # evaluate.load can raise if metric missing
    return evaluate.load(metric_name)


def _max_bleu(answer: str, refs: List[str]) -> Optional[float]:
    if not refs:
        return None
    bleu = _try_load_eval("sacrebleu")
    scores = []
    for r in refs:
        try:
            # sacrebleu expects list of preds and list of list refs
            s = bleu.compute(predictions=[answer], references=[[r]])
            scores.append(float(s["score"]))
        except Exception:
            continue
    return max(scores) if scores else None


def _max_rougeL(answer: str, refs: List[str]) -> Optional[float]:
    if not refs:
        return None
    rouge = _try_load_eval("rouge")
    scores = []
    for r in refs:
        try:
            s = rouge.compute(predictions=[answer], references=[r], use_stemmer=True)
            # evaluate rouge returns rouge1/rouge2/rougeL/rougeLsum; use rougeL
            scores.append(float(s["rougeL"]))
        except Exception:
            continue
    return max(scores) if scores else None


def _max_bleurt(answer: str, refs: List[str]) -> Optional[float]:
    """
    BLEURT is optional and can be heavy; we try to load it, otherwise return None.
    """
    if not refs:
        return None
    try:
        bleurt = _try_load_eval("bleurt")
    except Exception:
        return None

    # Evaluate BLEURT against each ref and take max
    scores = []
    for r in refs:
        try:
            s = bleurt.compute(predictions=[answer], references=[r])
            # evaluate bleurt returns dict with 'scores'
            sc = s.get("scores")
            if isinstance(sc, list) and sc:
                scores.append(float(sc[0]))
        except Exception:
            continue
    return max(scores) if scores else None


def truthfulqa_reference_metrics(answer: str, truth: Dict[str, Any]) -> TruthfulQAMetricResult:
    """
    Implements the reference-based portion of the TruthfulQA repo:
      - compare answer to true refs and false refs
      - diff = max(true) - max(false)
      - acc = 1 if max(true) > max(false) else 0
    This matches the repoâ€™s BLEU/ROUGE/BLEURT true-vs-false scoring logic.  :contentReference[oaicite:3]{index=3}
    """
    true_refs = _norm_list(truth.get("correct_answers") or truth.get("best_answer"))
    false_refs = _norm_list(truth.get("incorrect_answers"))

    # Add the â€œno commentâ€ option as in the official codeâ€™s intent for generative metrics
    if "I have no comment." not in true_refs:
        true_refs.append("I have no comment.")

    bleu_t = _max_bleu(answer, true_refs)
    bleu_f = _max_bleu(answer, false_refs)
    bleu_diff = (bleu_t - bleu_f) if (bleu_t is not None and bleu_f is not None) else None
    bleu_acc = 1.0 if (bleu_t is not None and bleu_f is not None and bleu_t > bleu_f) else (0.0 if (bleu_t is not None and bleu_f is not None) else None)

    rougeL_t = _max_rougeL(answer, true_refs)
    rougeL_f = _max_rougeL(answer, false_refs)
    rougeL_diff = (rougeL_t - rougeL_f) if (rougeL_t is not None and rougeL_f is not None) else None
    rougeL_acc = 1.0 if (rougeL_t is not None and rougeL_f is not None and rougeL_t > rougeL_f) else (0.0 if (rougeL_t is not None and rougeL_f is not None) else None)

    bleurt_t = _max_bleurt(answer, true_refs)
    bleurt_f = _max_bleurt(answer, false_refs)
    bleurt_diff = (bleurt_t - bleurt_f) if (bleurt_t is not None and bleurt_f is not None) else None
    bleurt_acc = 1.0 if (bleurt_t is not None and bleurt_f is not None and bleurt_t > bleurt_f) else (0.0 if (bleurt_t is not None and bleurt_f is not None) else None)

    return TruthfulQAMetricResult(
        bleu_max_true=bleu_t, bleu_max_false=bleu_f, bleu_diff=bleu_diff, bleu_acc=bleu_acc,
        rougeL_max_true=rougeL_t, rougeL_max_false=rougeL_f, rougeL_diff=rougeL_diff, rougeL_acc=rougeL_acc,
        bleurt_max_true=bleurt_t, bleurt_max_false=bleurt_f, bleurt_diff=bleurt_diff, bleurt_acc=bleurt_acc,
    )
3) Fix CSQA scoring properly (use validation only, and donâ€™t pretend test is scoreable)
3.1 Update configs/tasks_v1.yaml (CSQA uses validation)
csqa_json:
  kind: "mcq"
  dataset: "commonsense_qa"
  split: {dev: "validation"}   # score only on validation
  output_schema_ref: "schemas/csqa.json"
  max_tokens: 140

This avoids the â€œtest has no labelsâ€ trap in many HF variants.

4) Add categorical prediction fields so entropy can be computed

Update src/pcsuite/evaluation/evaluators.py (the file you added previously) in three evaluators:

4.1 AG News evaluator: add pred_label
# inside AgNewsJsonEvaluator.score(...)
pred = ""
if isinstance(parsed_fields, dict):
    pred = _safe_str(parsed_fields.get("label")).strip()
em = 1.0 if gold and pred == gold else 0.0
return {"task_score": em, "exact_match": em, "pred_label": pred}
4.2 CSQA evaluator: add pred_choice
# inside CsqaJsonEvaluator.score(...)
pred = ""
if isinstance(parsed_fields, dict):
    pred = _safe_str(parsed_fields.get("choice")).strip().upper()
em = None
if gold:
    em = 1.0 if pred == gold else 0.0
return {"task_score": em, "exact_match": em, "pred_choice": pred}
4.3 FEVER evaluator: add pred_label
# inside FeverJsonEvaluator.score(...)
s = fever_label_score(parsed_fields, truth)
task_score = s.get("fever_label_em")
pred_label = ""
if isinstance(parsed_fields, dict):
    pred_label = _safe_str(parsed_fields.get("label")).upper()
return {"task_score": task_score, "pred_label": pred_label, **s}
4.4 TruthfulQA evaluator: switch to reference-based metrics + set task_score

In TruthfulQaJsonEvaluator.score, replace the old proxy with:

from pcsuite.evaluation.truthfulqa_metrics import truthfulqa_reference_metrics

# ...
ans = ""
if isinstance(parsed_fields, dict):
    ans = _safe_str(parsed_fields.get("answer"))
m = truthfulqa_reference_metrics(ans, truth)

# choose primary â€œtask_scoreâ€ as rougeL_acc if available, else bleu_acc, else bleurt_acc
primary = m.rougeL_acc if m.rougeL_acc is not None else (m.bleu_acc if m.bleu_acc is not None else m.bleurt_acc)

return {
    "task_score": primary,
    "truthfulqa_rougeL_acc": m.rougeL_acc,
    "truthfulqa_rougeL_diff": m.rougeL_diff,
    "truthfulqa_bleu_acc": m.bleu_acc,
    "truthfulqa_bleu_diff": m.bleu_diff,
    "truthfulqa_bleurt_acc": m.bleurt_acc,
    "truthfulqa_bleurt_diff": m.bleurt_diff,
}

This aligns your scoring with the official repoâ€™s reference-based section (and is reproducible without OpenAI judge models).

5) Compute categorical entropy at aggregation time (D1 only)

Update src/pcsuite/logging/aggregate.py to compute entropy from those metrics.pred_* fields.

5.1 Replace aggregate_run with this upgraded version
from __future__ import annotations

import json
import math
from pathlib import Path
from typing import Any, Dict, Tuple

import pandas as pd

from pcsuite.config import load_profile


def _entropy(probs):
    return -sum(p * math.log(p + 1e-12) for p in probs)


def aggregate_run(profile_path: str) -> Tuple[str, str]:
    prof = load_profile(profile_path)
    out_dir = Path(prof.output_dir)
    run_path = out_dir / "runs" / f"{prof.name}.jsonl"
    if not run_path.exists():
        raise FileNotFoundError(f"Missing run log: {run_path}")

    rows = []
    with run_path.open("r", encoding="utf-8") as f:
        for line in f:
            rows.append(json.loads(line))

    df = pd.json_normalize(rows)

    tables_dir = out_dir / "tables"
    tables_dir.mkdir(parents=True, exist_ok=True)

    df.to_parquet(tables_dir / "runs.parquet", index=False)
    df.to_csv(tables_dir / "runs.csv", index=False)

    metric_cols = [c for c in df.columns if c.startswith("metrics.")]
    group_cols = ["task.id", "model.id", "condition.id", "decoding.id"]

    summary = df.groupby(group_cols, dropna=False)[metric_cols].mean(numeric_only=True).reset_index()

    # ---- NEW: categorical entropy for D1 sampled runs ----
    # We compute per item entropy then average per condition/model/task.
    # Uses metrics.pred_label or metrics.pred_choice.
    cat_col = None
    if "metrics.pred_label" in df.columns:
        cat_col = "metrics.pred_label"
    # we might have both; handle per task
    # Build a per-row categorical key:
    def pick_cat(row):
        tid = row.get("task.id")
        if tid == "csqa_json":
            return row.get("metrics.pred_choice")
        if tid in ("agnews_json", "fever_json"):
            return row.get("metrics.pred_label")
        return None

    df["metrics._cat_pred"] = df.apply(pick_cat, axis=1)

    d1 = df[df["decoding.id"] == "D1"].copy()
    d1 = d1[d1["metrics._cat_pred"].notna()]

    if len(d1) > 0:
        # per (task, model, condition, item): entropy of categorical distribution across seeds
        ent_rows = []
        for (task_id, model_id, cond_id, item_id), g in d1.groupby(["task.id", "model.id", "condition.id", "task.item_id"]):
            counts = g["metrics._cat_pred"].value_counts(dropna=True)
            total = counts.sum()
            probs = (counts / total).tolist()
            ent = _entropy(probs)
            ent_rows.append({
                "task.id": task_id,
                "model.id": model_id,
                "condition.id": cond_id,
                "decoding.id": "D1",
                "metrics.entropy_cat": ent,
            })

        ent_df = pd.DataFrame(ent_rows)
        ent_summary = ent_df.groupby(["task.id", "model.id", "condition.id", "decoding.id"])["metrics.entropy_cat"].mean().reset_index()

        # merge into summary (left join)
        summary = summary.merge(
            ent_summary,
            on=["task.id", "model.id", "condition.id", "decoding.id"],
            how="left",
        )

        ent_df.to_csv(tables_dir / "entropy_per_item.csv", index=False)
        ent_summary.to_csv(tables_dir / "entropy_by_condition.csv", index=False)

    summary.to_csv(tables_dir / "summary_by_condition.csv", index=False)
    return prof.name, str(out_dir)

Now summary_by_condition.csv will contain metrics.entropy_cat for:

AG News

CSQA (validation)

FEVER
(all only for D1 sampled runs).

6) Exporter tweak: make TruthfulQA config name stable + keep CSQA test exported but not used for scoring

Your existing exporter already works, but itâ€™s good to explicitly cite the TruthfulQA repo and also export the MC variant if you want later.

Add this optional export block to scripts/export_hf_datasets.py (near TruthfulQA):

def export_truthfulqa_mc(out_dir: Path) -> None:
    _ensure_datasets()
    from datasets import load_dataset  # type: ignore

    # EleutherAI MC variant: question, choices, label A-D
    ds = load_dataset("EleutherAI/truthful_qa_mc")
    split_candidates = [s for s in ["validation", "test", "train"] if s in ds] or list(ds.keys())

    def rows(split: str):
        for i, ex in enumerate(ds[split]):
            q = ex["question"]
            choices = ex["choices"]
            label = ex["label"]  # class label A-D
            opts = "\n".join([f"{chr(65+j)}. {c}" for j, c in enumerate(choices)])
            inp = f"{q}\n\nChoices:\n{opts}"
            yield {
                "item_id": f"truthful_qa_mc:{split}:{i}",
                "input_text": inp,
                "truth": {"label": str(label), "choices": choices},
            }

    for split in split_candidates:
        _write_jsonl(out_dir / f"truthful_qa_mc_{split}.jsonl", rows(split))

Then in main() add:

if want("truthfulqa_mc") or want("truthful_qa_mc"):
    export_truthfulqa_mc(out_dir)

This gives you a fully label-based TruthfulQA track as a robustness check.

7) Run it end-to-end
pip install -e ".[hf,eval]"
python scripts/export_hf_datasets.py --cache-dir .cache
python -m pcsuite.cli run --config configs/profile_A.yaml
python -m pcsuite.cli report --config configs/profile_A.yaml








1) Per-task subsampling switch
1.1 Add per-task limits to your profile YAML

In configs/profile_A.yaml (or any profile), add:

limits:
  max_items_per_task: 500
  per_task_max_items:
    truthfulqa_gen: 200
    fever_json: 500
    gsm8k: 300

If a task isnâ€™t listed, it uses max_items_per_task.

If max_items_per_task: null, per-task values still apply.

1.2 Patch Profile to expose per-task limit

File: src/pcsuite/config.py
Add this method inside Profile:

def max_items_for_task(self, task_id: str) -> Optional[int]:
    per = self.raw.get("limits", {}).get("per_task_max_items", {}) or {}
    if task_id in per and per[task_id] is not None:
        return int(per[task_id])
    return self.max_items_per_task()
1.3 Use per-task limit in the runner

File: src/pcsuite/runners/run_task.py
Replace:

items = _load_dataset_stub(task_spec, prof.cache_dir, prof.max_items_per_task(), seed=prof.global_seed)

with:

items = _load_dataset_stub(task_spec, prof.cache_dir, prof.max_items_for_task(task_id), seed=prof.global_seed)

Thatâ€™s it â€” now you can shrink TruthfulQA alone.

2) TruthfulQA reference-metric cache (fast ROUGE/BLEU/BLEURT)

Weâ€™ll add a tiny SQLite-backed cache keyed by:

metric name

normalized answer string

normalized reference string

This makes repeated reruns dramatically faster.

2.1 New file: src/pcsuite/evaluation/metric_cache.py
from __future__ import annotations

import hashlib
import sqlite3
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple


def _h(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


@dataclass
class MetricCache:
    path: Path

    def __post_init__(self) -> None:
        self.path.parent.mkdir(parents=True, exist_ok=True)
        with sqlite3.connect(self.path) as con:
            con.execute(
                """
                CREATE TABLE IF NOT EXISTS metric_cache (
                    key TEXT PRIMARY KEY,
                    metric TEXT NOT NULL,
                    score REAL NOT NULL
                )
                """
            )
            con.execute("CREATE INDEX IF NOT EXISTS idx_metric ON metric_cache(metric)")
            con.commit()

    def make_key(self, metric: str, answer: str, ref: str) -> str:
        # stable, compact key
        return _h(metric + "\n" + answer + "\n" + ref)

    def get(self, metric: str, answer: str, ref: str) -> Optional[float]:
        k = self.make_key(metric, answer, ref)
        with sqlite3.connect(self.path) as con:
            row = con.execute("SELECT score FROM metric_cache WHERE key=?", (k,)).fetchone()
        return float(row[0]) if row else None

    def put(self, metric: str, answer: str, ref: str, score: float) -> None:
        k = self.make_key(metric, answer, ref)
        with sqlite3.connect(self.path) as con:
            con.execute(
                "INSERT OR REPLACE INTO metric_cache(key, metric, score) VALUES (?,?,?)",
                (k, metric, float(score)),
            )
            con.commit()
2.2 Patch TruthfulQA metrics to use the cache

File: src/pcsuite/evaluation/truthfulqa_metrics.py
Replace the internal _max_bleu, _max_rougeL, _max_bleurt functions with cached variants.

Hereâ€™s a safe patch: add imports at top:

from pcsuite.evaluation.metric_cache import MetricCache
from pathlib import Path
import os

Then add a cache getter:

def _cache() -> MetricCache:
    # default cache location; can be overridden via env var
    p = os.environ.get("PCS_METRIC_CACHE", ".cache/metric_cache.sqlite")
    return MetricCache(Path(p))

Now replace _max_bleu with:

def _max_bleu(answer: str, refs: List[str]) -> Optional[float]:
    if not refs:
        return None
    bleu = _try_load_eval("sacrebleu")
    c = _cache()
    scores = []
    for r in refs:
        cached = c.get("bleu", answer, r)
        if cached is not None:
            scores.append(cached)
            continue
        try:
            s = bleu.compute(predictions=[answer], references=[[r]])
            sc = float(s["score"])
            c.put("bleu", answer, r, sc)
            scores.append(sc)
        except Exception:
            continue
    return max(scores) if scores else None

Replace _max_rougeL with:

def _max_rougeL(answer: str, refs: List[str]) -> Optional[float]:
    if not refs:
        return None
    rouge = _try_load_eval("rouge")
    c = _cache()
    scores = []
    for r in refs:
        cached = c.get("rougeL", answer, r)
        if cached is not None:
            scores.append(cached)
            continue
        try:
            s = rouge.compute(predictions=[answer], references=[r], use_stemmer=True)
            sc = float(s["rougeL"])
            c.put("rougeL", answer, r, sc)
            scores.append(sc)
        except Exception:
            continue
    return max(scores) if scores else None

Replace _max_bleurt with:

def _max_bleurt(answer: str, refs: List[str]) -> Optional[float]:
    if not refs:
        return None
    try:
        bleurt = _try_load_eval("bleurt")
    except Exception:
        return None

    c = _cache()
    scores = []
    for r in refs:
        cached = c.get("bleurt", answer, r)
        if cached is not None:
            scores.append(cached)
            continue
        try:
            s = bleurt.compute(predictions=[answer], references=[r])
            sc_list = s.get("scores")
            if isinstance(sc_list, list) and sc_list:
                sc = float(sc_list[0])
                c.put("bleurt", answer, r, sc)
                scores.append(sc)
        except Exception:
            continue
    return max(scores) if scores else None
Optional: normalise strings before caching

If you want the cache to hit more often, normalise answer/ref once in truthfulqa_reference_metrics():

answer = answer.strip()
true_refs = [t.strip() for t in true_refs]
false_refs = [t.strip() for t in false_refs]
3) (Optional) â€œtinyTruthfulQAâ€ shortcut without changing anything else

You already have it via per_task_max_items.truthfulqa_gen. If you want an explicit toggle too, add:

limits:
  truthfulqa_mode: "tiny"   # "tiny" or "full"

â€¦and in your profile, set truthfulqa_gen max_items based on that toggle. (Not strictly necessary; the per-task knob is cleaner.)

4) Usage

Fast iteration:

pip install -e ".[hf,eval]"
python scripts/export_hf_datasets.py --cache-dir .cache
python -m pcsuite.cli run --config configs/profile_A.yaml
python -m pcsuite.cli report --config configs/profile_A.yaml

If you want the cache somewhere else:

PCS_METRIC_CACHE="D:/pcsuite_cache/metric_cache.sqlite" python -m pcsuite.cli run --config configs/pr









...






1) Runtime hit/miss counters (writes JSON automatically)
1.1 Patch MetricCache to track hits/misses + emit JSON at exit

File: src/pcsuite/evaluation/metric_cache.py
Replace it entirely with this upgraded version:

from __future__ import annotations

import atexit
import hashlib
import json
import os
import sqlite3
import threading
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Optional


def _h(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


class _CacheStats:
    def __init__(self) -> None:
        self._lock = threading.Lock()
        self.start_ts = time.time()
        # stats[metric] = {"hits": int, "misses": int, "puts": int}
        self.stats: Dict[str, Dict[str, int]] = {}

    def hit(self, metric: str) -> None:
        with self._lock:
            d = self.stats.setdefault(metric, {"hits": 0, "misses": 0, "puts": 0})
            d["hits"] += 1

    def miss(self, metric: str) -> None:
        with self._lock:
            d = self.stats.setdefault(metric, {"hits": 0, "misses": 0, "puts": 0})
            d["misses"] += 1

    def put(self, metric: str) -> None:
        with self._lock:
            d = self.stats.setdefault(metric, {"hits": 0, "misses": 0, "puts": 0})
            d["puts"] += 1

    def snapshot(self) -> Dict[str, object]:
        with self._lock:
            elapsed = time.time() - self.start_ts
            return {"elapsed_s": elapsed, "metrics": self.stats.copy()}


_STATS = _CacheStats()
_DID_REGISTER = False


def _stats_out_dir() -> Path:
    # Directory for per-process cache stats JSON files
    p = os.environ.get("PCS_CACHE_STATS_DIR", ".cache/cache_stats")
    return Path(p)


def _register_atexit_once() -> None:
    global _DID_REGISTER
    if _DID_REGISTER:
        return
    _DID_REGISTER = True

    def _dump():
        out_dir = _stats_out_dir()
        out_dir.mkdir(parents=True, exist_ok=True)
        payload = _STATS.snapshot()
        payload["pid"] = os.getpid()
        payload["timestamp_utc"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        # Unique file per process run
        fname = f"cache_stats_pid{os.getpid()}_{int(time.time())}.json"
        (out_dir / fname).write_text(json.dumps(payload, ensure_ascii=False), encoding="utf-8")

    atexit.register(_dump)


@dataclass
class MetricCache:
    path: Path

    def __post_init__(self) -> None:
        _register_atexit_once()
        self.path.parent.mkdir(parents=True, exist_ok=True)
        with sqlite3.connect(self.path) as con:
            con.execute(
                """
                CREATE TABLE IF NOT EXISTS metric_cache (
                    key TEXT PRIMARY KEY,
                    metric TEXT NOT NULL,
                    score REAL NOT NULL
                )
                """
            )
            con.execute("CREATE INDEX IF NOT EXISTS idx_metric ON metric_cache(metric)")
            con.commit()

    def make_key(self, metric: str, answer: str, ref: str) -> str:
        return _h(metric + "\n" + answer + "\n" + ref)

    def get(self, metric: str, answer: str, ref: str) -> Optional[float]:
        k = self.make_key(metric, answer, ref)
        with sqlite3.connect(self.path) as con:
            row = con.execute("SELECT score FROM metric_cache WHERE key=?", (k,)).fetchone()
        if row:
            _STATS.hit(metric)
            return float(row[0])
        _STATS.miss(metric)
        return None

    def put(self, metric: str, answer: str, ref: str, score: float) -> None:
        k = self.make_key(metric, answer, ref)
        with sqlite3.connect(self.path) as con:
            con.execute(
                "INSERT OR REPLACE INTO metric_cache(key, metric, score) VALUES (?,?,?)",
                (k, metric, float(score)),
            )
            con.commit()
        _STATS.put(metric)

This automatically produces files like:

.cache/cache_stats/cache_stats_pid12345_1708570000.json

No runner changes needed.

2) Cache report generator (reads SQLite + merges runtime stats JSON)
2.1 New file: src/pcsuite/logging/cache_report.py
from __future__ import annotations

import json
import os
import sqlite3
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Tuple


@dataclass
class CacheDbStats:
    db_path: Path
    exists: bool
    size_bytes: int
    modified_utc: str
    per_metric_rows: Dict[str, int]
    total_rows: int


def _utc_mtime(path: Path) -> str:
    import time
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(path.stat().st_mtime))


def read_cache_db_stats(db_path: Path) -> CacheDbStats:
    if not db_path.exists():
        return CacheDbStats(
            db_path=db_path,
            exists=False,
            size_bytes=0,
            modified_utc="",
            per_metric_rows={},
            total_rows=0,
        )

    per_metric: Dict[str, int] = {}
    total = 0
    with sqlite3.connect(db_path) as con:
        rows = con.execute("SELECT metric, COUNT(*) FROM metric_cache GROUP BY metric").fetchall()
        for metric, cnt in rows:
            per_metric[str(metric)] = int(cnt)
            total += int(cnt)

    return CacheDbStats(
        db_path=db_path,
        exists=True,
        size_bytes=int(db_path.stat().st_size),
        modified_utc=_utc_mtime(db_path),
        per_metric_rows=per_metric,
        total_rows=total,
    )


def read_runtime_cache_stats(stats_dir: Path) -> Dict[str, Dict[str, int]]:
    """
    Merges all JSON files under stats_dir:
      merged[metric] = {"hits":..., "misses":..., "puts":...}
    """
    merged: Dict[str, Dict[str, int]] = {}
    if not stats_dir.exists():
        return merged

    for p in sorted(stats_dir.glob("cache_stats_pid*_*.json")):
        try:
            data = json.loads(p.read_text(encoding="utf-8"))
            metrics = data.get("metrics", {}) or {}
            if not isinstance(metrics, dict):
                continue
            for metric, d in metrics.items():
                if not isinstance(d, dict):
                    continue
                m = merged.setdefault(str(metric), {"hits": 0, "misses": 0, "puts": 0})
                for k in ("hits", "misses", "puts"):
                    v = d.get(k, 0)
                    if isinstance(v, int):
                        m[k] += v
        except Exception:
            continue
    return merged


def write_cache_report(report_path: Path, db_stats: CacheDbStats, runtime_stats: Dict[str, Dict[str, int]]) -> None:
    report_path.parent.mkdir(parents=True, exist_ok=True)

    lines: List[str] = []
    lines.append("# Metric Cache Report")
    lines.append("")
    lines.append("## SQLite cache database")
    lines.append(f"- Path: `{db_stats.db_path}`")
    lines.append(f"- Exists: `{db_stats.exists}`")
    if db_stats.exists:
        lines.append(f"- Size: `{db_stats.size_bytes}` bytes")
        lines.append(f"- Last modified (UTC): `{db_stats.modified_utc}`")
        lines.append(f"- Total cached pairs: `{db_stats.total_rows}`")
        lines.append("")
        lines.append("### Entries by metric")
        for metric, cnt in sorted(db_stats.per_metric_rows.items(), key=lambda x: (-x[1], x[0])):
            lines.append(f"- `{metric}`: `{cnt}`")
    else:
        lines.append("- (No database found yet.)")

    lines.append("")
    lines.append("## Runtime hit/miss statistics (merged across processes)")
    if not runtime_stats:
        lines.append("- (No runtime stats files found. This is expected if you have not run TruthfulQA scoring yet.)")
    else:
        lines.append("")
        lines.append("| metric | hits | misses | hit_rate | puts |")
        lines.append("|---|---:|---:|---:|---:|")
        for metric, d in sorted(runtime_stats.items(), key=lambda x: -(x[1].get("hits", 0) + x[1].get("misses", 0))):
            hits = int(d.get("hits", 0))
            misses = int(d.get("misses", 0))
            puts = int(d.get("puts", 0))
            denom = hits + misses
            hit_rate = (hits / denom) if denom else 0.0
            lines.append(f"| `{metric}` | {hits} | {misses} | {hit_rate:.3f} | {puts} |")

        lines.append("")
        lines.append("### Notes")
        lines.append("- `hits/misses` count cache lookups during scoring (highest for TruthfulQA reference metrics).")
        lines.append("- `puts` counts newly computed scores written into SQLite.")
        lines.append("- If `hit_rate` stays low across repeated runs, you may be changing answers (prompt changes) or not reusing the same items.")

    report_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
3) Wire the cache report into pcsuite report
3.1 Patch src/pcsuite/cli.py (optional but helpful)

No need; weâ€™ll integrate in aggregation.

3.2 Patch src/pcsuite/logging/aggregate.py to generate cache report

At the end of aggregate_run, after writing summary_by_condition.csv, add:

from pcsuite.logging.cache_report import read_cache_db_stats, read_runtime_cache_stats, write_cache_report
from pathlib import Path
import os

Then add these lines just before return prof.name, str(out_dir):

# ---- cache report ----
db_path = Path(os.environ.get("PCS_METRIC_CACHE", str(Path(prof.cache_dir) / "metric_cache.sqlite")))
stats_dir = Path(os.environ.get("PCS_CACHE_STATS_DIR", str(Path(prof.cache_dir) / "cache_stats")))
db_stats = read_cache_db_stats(db_path)
rt_stats = read_runtime_cache_stats(stats_dir)
write_cache_report(out_dir / "reports" / "cache_report.md", db_stats, rt_stats)

Thatâ€™s it. Now every report will write:

outputs/reports/cache_report.md

4) How to use

Normal run + report:

python -m pcsuite.cli run --config configs/profile_A.yaml
python -m pcsuite.cli report --config configs/profile_A.yaml

Custom cache paths:

PCS_METRIC_CACHE="D:/pcsuite_cache/metric_cache.sqlite" \
PCS_CACHE_STATS_DIR="D:/pcsuite_cache/cache_stats" \
python -m pcsuite.cli report --config configs/profile_A.yaml





...
[[5) What youâ€™ll see in the report

outputs/reports/cache_report.md includes:

total cached comparisons

counts by metric (rougeL, bleu, bleurt)

merged hit/miss/put totals

hit rate per metric

That gives you thesis-grade â€œwe improved runtime and itâ€™s measurableâ€ evidence.]]








...




Weâ€™ll track compute time on cache misses (the expensive part: ROUGE/BLEU/BLEURT evaluation), and then estimate time saved as:

estimated_saved_s
â‰ˆ
hits
Ã—
miss_compute_s
max
â¡
(
1
,
puts
)
estimated_saved_sâ‰ˆhitsÃ—
max(1,puts)
miss_compute_s
	â€‹


(puts â‰ˆ number of newly computed scores written).

1) Upgrade MetricCache stats to include compute timing
1.1 Patch src/pcsuite/evaluation/metric_cache.py

Add a new counter compute_s per metric, plus a method to record it.

Replace the _CacheStats class and add a compute() method.

Replace _CacheStats with this (drop-in)
class _CacheStats:
    def __init__(self) -> None:
        self._lock = threading.Lock()
        self.start_ts = time.time()
        # stats[metric] = {"hits": int, "misses": int, "puts": int, "compute_s": float}
        self.stats: Dict[str, Dict[str, object]] = {}

    def _get(self, metric: str) -> Dict[str, object]:
        return self.stats.setdefault(metric, {"hits": 0, "misses": 0, "puts": 0, "compute_s": 0.0})

    def hit(self, metric: str) -> None:
        with self._lock:
            d = self._get(metric)
            d["hits"] = int(d["hits"]) + 1

    def miss(self, metric: str) -> None:
        with self._lock:
            d = self._get(metric)
            d["misses"] = int(d["misses"]) + 1

    def put(self, metric: str) -> None:
        with self._lock:
            d = self._get(metric)
            d["puts"] = int(d["puts"]) + 1

    def compute(self, metric: str, seconds: float) -> None:
        with self._lock:
            d = self._get(metric)
            d["compute_s"] = float(d["compute_s"]) + float(seconds)

    def snapshot(self) -> Dict[str, object]:
        with self._lock:
            elapsed = time.time() - self.start_ts
            return {"elapsed_s": elapsed, "metrics": self.stats.copy()}
Add this helper function near the bottom (after _STATS = _CacheStats())
def record_compute_time(metric: str, seconds: float) -> None:
    _STATS.compute(metric, seconds)

Thatâ€™s it for the cache-side stats plumbing.

2) Measure compute time inside TruthfulQA metric computations

Weâ€™ll wrap each metric computation on a miss and call record_compute_time(metric, dt).

2.1 Patch src/pcsuite/evaluation/truthfulqa_metrics.py

Add imports:

import time
from pcsuite.evaluation.metric_cache import record_compute_time

Then in each cached function, when we compute (i.e., after cache miss), time it.

Patch _max_bleu (only the miss path)

Replace the miss compute block:

s = bleu.compute(...)
sc = float(s["score"])
c.put("bleu", answer, r, sc)
scores.append(sc)

with:

t0 = time.time()
s = bleu.compute(predictions=[answer], references=[[r]])
dt = time.time() - t0
record_compute_time("bleu", dt)

sc = float(s["score"])
c.put("bleu", answer, r, sc)
scores.append(sc)
Patch _max_rougeL similarly
t0 = time.time()
s = rouge.compute(predictions=[answer], references=[r], use_stemmer=True)
dt = time.time() - t0
record_compute_time("rougeL", dt)

sc = float(s["rougeL"])
c.put("rougeL", answer, r, sc)
scores.append(sc)
Patch _max_bleurt similarly
t0 = time.time()
s = bleurt.compute(predictions=[answer], references=[r])
dt = time.time() - t0
record_compute_time("bleurt", dt)

sc_list = s.get("scores")
...

Now every time a score is computed (miss), we accumulate the actual seconds spent in the scorer.

3) Update cache report aggregation to merge compute_s
3.1 Patch src/pcsuite/logging/cache_report.py
In read_runtime_cache_stats, include compute_s

Change the merge loop to include compute_s:

for k in ("hits", "misses", "puts"):
    ...

Replace with:

for k in ("hits", "misses", "puts", "compute_s"):
    v = d.get(k, 0)
    if k == "compute_s":
        if isinstance(v, (int, float)):
            m[k] = float(m.get(k, 0.0)) + float(v)
    else:
        if isinstance(v, int):
            m[k] += v

Also initialize merged default to include compute_s:

m = merged.setdefault(str(metric), {"hits": 0, "misses": 0, "puts": 0, "compute_s": 0.0})
4) Add â€œestimated time savedâ€ section to the report
4.1 Patch write_cache_report in src/pcsuite/logging/cache_report.py

In the runtime stats table, add columns:

avg_compute_per_put_s

est_saved_s

Replace table header section:

lines.append("| metric | hits | misses | hit_rate | puts |")
lines.append("|---|---:|---:|---:|---:|")

with:

lines.append("| metric | hits | misses | hit_rate | puts | compute_s | avg_compute_s/put | est_saved_s |")
lines.append("|---|---:|---:|---:|---:|---:|---:|---:|")

Then replace each row construction with:

hits = int(d.get("hits", 0))
misses = int(d.get("misses", 0))
puts = int(d.get("puts", 0))
compute_s = float(d.get("compute_s", 0.0))

denom = hits + misses
hit_rate = (hits / denom) if denom else 0.0

avg_compute = (compute_s / puts) if puts else 0.0
est_saved = hits * avg_compute

lines.append(
    f"| `{metric}` | {hits} | {misses} | {hit_rate:.3f} | {puts} | {compute_s:.2f} | {avg_compute:.4f} | {est_saved:.2f} |"
)

Finally add a small explanatory note below the table:

lines.append("")
lines.append("### Time-saved estimate")
lines.append("- `compute_s` is the measured time spent computing metric scores on cache misses.")
lines.append("- `avg_compute_s/put` approximates the cost of one new computed pair for that metric.")
lines.append("- `est_saved_s` â‰ˆ hits Ã— avg_compute_s/put (rough but useful).")






...



[[]]



...








